# 逻辑回归（Logistic Regression）：原理、估计与应用

## 目录
1. [逻辑回归概述](#一-逻辑回归概述)
2. [二分类逻辑回归](#二-二分类逻辑回归)
3. [多分类逻辑回归](#三-多分类逻辑回归)
4. [参数估计：最大似然估计](#四-参数估计最大似然估计)
5. [模型评估](#五-模型评估)
6. [正则化逻辑回归](#六-正则化逻辑回归)
7. [逻辑回归的假设与诊断](#七-逻辑回归的假设与诊断)
8. [在量化交易中的应用](#八-在量化交易中的应用)
9. [Python 实现示例](#九-python-实现示例)
10. [总结](#十-总结)

---

## 一、逻辑回归概述

### 1.1 什么是逻辑回归

**逻辑回归（Logistic Regression）**是一种用于分类问题的统计学习方法，虽然名字中有"回归"，但它实际上是一种**分类算法**。

**核心思想：**
- 使用**逻辑函数**（Sigmoid 函数）将线性组合映射到 $[0, 1]$ 区间
- 输出可以解释为**概率**
- 通过**最大似然估计**估计参数

### 1.2 为什么需要逻辑回归

**线性回归的局限性：**

对于分类问题，直接使用线性回归会遇到问题：

1. **输出范围问题**
   - 线性回归输出可以是任意实数
   - 分类问题需要输出概率（$[0, 1]$ 区间）

2. **假设不满足**
   - 线性回归假设误差项服从正态分布
   - 分类问题的误差项不满足正态分布

3. **预测值不合理**
   - 线性回归可能输出小于 0 或大于 1 的值
   - 无法解释为概率

**逻辑回归的解决方案：**
- 使用逻辑函数将输出映射到 $[0, 1]$
- 输出可以解释为概率
- 适用于分类问题

### 1.3 逻辑回归的类型

**1. 二分类逻辑回归（Binary Logistic Regression）**
- 两个类别（如：涨/跌、是/否）
- 最常用的形式

**2. 多分类逻辑回归（Multinomial Logistic Regression）**
- 多个类别（如：上涨/横盘/下跌）
- 使用 Softmax 函数

**3. 有序逻辑回归（Ordinal Logistic Regression）**
- 有序的多个类别（如：低风险/中风险/高风险）
- 考虑类别的顺序关系

### 1.4 逻辑回归的应用

**主要应用领域：**
1. **二分类问题**：垃圾邮件检测、疾病诊断、信用评分
2. **多分类问题**：图像分类、文本分类
3. **概率预测**：预测事件发生的概率
4. **量化交易**：涨跌预测、交易信号生成、风险管理

### 1.5 逻辑回归的优势

**主要优势：**
- ✅ **输出概率**：可以直接输出概率，便于解释
- ✅ **计算高效**：训练和预测速度快
- ✅ **可解释性强**：系数有明确的含义
- ✅ **不需要特征缩放**：对特征的尺度不敏感
- ✅ **理论基础**：有完整的统计理论支撑

---

## 二、二分类逻辑回归

### 2.1 模型定义

**二分类逻辑回归模型：**

$$
P(Y = 1 | \mathbf{X}) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p)}} = \sigma(\mathbf{X}'\boldsymbol{\beta})
$$

其中：
- $Y \in \{0, 1\}$：二分类响应变量
- $\mathbf{X} = (1, X_1, X_2, \ldots, X_p)'$：特征向量
- $\boldsymbol{\beta} = (\beta_0, \beta_1, \ldots, \beta_p)'$：参数向量
- $\sigma(z) = \frac{1}{1 + e^{-z}}$：逻辑函数（Sigmoid 函数）

### 2.2 逻辑函数（Sigmoid 函数）

**定义：**

$$
\sigma(z) = \frac{1}{1 + e^{-z}} = \frac{e^z}{1 + e^z}
$$

**性质：**

1. **值域**：$\sigma(z) \in (0, 1)$
2. **单调性**：$\sigma'(z) = \sigma(z)(1 - \sigma(z)) > 0$（单调递增）
3. **对称性**：$\sigma(-z) = 1 - \sigma(z)$
4. **极限**：
   - $\lim_{z \to -\infty} \sigma(z) = 0$
   - $\lim_{z \to +\infty} \sigma(z) = 1$
   - $\sigma(0) = 0.5$

**图像特征：**
- S 形曲线
- 在 $z = 0$ 处斜率最大
- 两端趋于水平

### 2.3 几率（Odds）和对数几率（Logit）

**几率（Odds）：**

$$
\text{Odds} = \frac{P(Y = 1 | \mathbf{X})}{P(Y = 0 | \mathbf{X})} = \frac{P(Y = 1 | \mathbf{X})}{1 - P(Y = 1 | \mathbf{X})}
$$

**对数几率（Logit）：**

$$
\text{Logit}(P) = \ln\left(\frac{P}{1 - P}\right)
$$

**逻辑回归的线性形式：**

$$
\text{Logit}(P(Y = 1 | \mathbf{X})) = \ln\left(\frac{P(Y = 1 | \mathbf{X})}{1 - P(Y = 1 | \mathbf{X})}\right) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p
$$

**解释：**
- 逻辑回归是对数几率的线性模型
- 系数 $\beta_j$ 表示 $X_j$ 每增加一个单位，对数几率的变化量

### 2.4 参数的含义

**截距项 $\beta_0$：**
- 当所有特征为 0 时，对数几率的值
- 表示基线对数几率

**系数 $\beta_j$（$j = 1, 2, \ldots, p$）：**
- 在控制其他变量不变的情况下，$X_j$ 每增加一个单位，对数几率的变化量
- $\beta_j > 0$：$X_j$ 增加时，$P(Y = 1)$ 增加
- $\beta_j < 0$：$X_j$ 增加时，$P(Y = 1)$ 减少
- $\beta_j = 0$：$X_j$ 对 $P(Y = 1)$ 无影响

**几率比（Odds Ratio）：**

$$
\text{OR}_j = e^{\beta_j} = \frac{\text{Odds}(X_j + 1)}{\text{Odds}(X_j)}
$$

- $\text{OR}_j > 1$：$X_j$ 增加一个单位，几率增加
- $\text{OR}_j < 1$：$X_j$ 增加一个单位，几率减少
- $\text{OR}_j = 1$：$X_j$ 对几率无影响

### 2.5 决策边界

**决策规则：**

$$
\hat{Y} = \begin{cases}
1 & \text{if } P(Y = 1 | \mathbf{X}) \geq 0.5 \\
0 & \text{if } P(Y = 1 | \mathbf{X}) < 0.5
\end{cases}
$$

**决策边界：**

当 $P(Y = 1 | \mathbf{X}) = 0.5$ 时：

$$
\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p = 0
$$

这是一个**线性决策边界**。

**特点：**
- 决策边界是线性的（超平面）
- 在二维情况下是一条直线
- 在三维情况下是一个平面

---

## 三、多分类逻辑回归

### 3.1 多分类问题

**问题设置：**

- $K$ 个类别：$Y \in \{1, 2, \ldots, K\}$
- 需要估计 $K$ 个类别的概率

### 3.2 Softmax 回归

**Softmax 函数：**

对于 $K$ 个类别，Softmax 函数定义为：

$$
P(Y = k | \mathbf{X}) = \frac{e^{\mathbf{X}'\boldsymbol{\beta}_k}}{\sum_{j=1}^{K} e^{\mathbf{X}'\boldsymbol{\beta}_j}}, \quad k = 1, 2, \ldots, K
$$

其中 $\boldsymbol{\beta}_k$ 是第 $k$ 类的参数向量。

**性质：**
- $\sum_{k=1}^{K} P(Y = k | \mathbf{X}) = 1$（概率和为 1）
- $P(Y = k | \mathbf{X}) \in (0, 1)$（每个概率在 0 和 1 之间）

### 3.3 参数识别

**问题：** 参数不唯一（需要约束）

**解决方案：** 选择第 $K$ 类作为参考类别

$$
P(Y = k | \mathbf{X}) = \frac{e^{\mathbf{X}'\boldsymbol{\beta}_k}}{1 + \sum_{j=1}^{K-1} e^{\mathbf{X}'\boldsymbol{\beta}_j}}, \quad k = 1, 2, \ldots, K-1
$$

$$
P(Y = K | \mathbf{X}) = \frac{1}{1 + \sum_{j=1}^{K-1} e^{\mathbf{X}'\boldsymbol{\beta}_j}}
$$

**参数数量：** $(K-1) \times (p+1)$

### 3.4 多分类逻辑回归的估计

**对数似然函数：**

$$
\ell(\boldsymbol{\beta}) = \sum_{i=1}^{n} \sum_{k=1}^{K} \mathbb{I}(y_i = k) \ln P(Y_i = k | \mathbf{X}_i)
$$

使用迭代优化算法（如 Newton-Raphson）求解。

---

## 四、参数估计：最大似然估计

### 4.1 似然函数

**二分类逻辑回归的似然函数：**

对于 $n$ 个独立观测 $(y_i, \mathbf{x}_i)$，$i = 1, 2, \ldots, n$：

$$
L(\boldsymbol{\beta}) = \prod_{i=1}^{n} P(Y_i = y_i | \mathbf{X}_i) = \prod_{i=1}^{n} [\pi_i^{y_i}(1 - \pi_i)^{1 - y_i}]
$$

其中 $\pi_i = P(Y_i = 1 | \mathbf{X}_i) = \sigma(\mathbf{X}_i'\boldsymbol{\beta})$。

### 4.2 对数似然函数

**对数似然函数：**

$$
\ell(\boldsymbol{\beta}) = \ln L(\boldsymbol{\beta}) = \sum_{i=1}^{n} [y_i \ln \pi_i + (1 - y_i) \ln(1 - \pi_i)]
$$

**展开形式：**

$$
\ell(\boldsymbol{\beta}) = \sum_{i=1}^{n} \left[y_i (\mathbf{X}_i'\boldsymbol{\beta}) - \ln(1 + e^{\mathbf{X}_i'\boldsymbol{\beta}})\right]
$$

### 4.3 最大似然估计

**目标：** 最大化对数似然函数

$$
\hat{\boldsymbol{\beta}} = \arg\max_{\boldsymbol{\beta}} \ell(\boldsymbol{\beta})
$$

**等价于最小化负对数似然：**

$$
\hat{\boldsymbol{\beta}} = \arg\min_{\boldsymbol{\beta}} -\ell(\boldsymbol{\beta})
$$

### 4.4 梯度

**一阶导数（梯度）：**

$$
\frac{\partial \ell}{\partial \boldsymbol{\beta}} = \sum_{i=1}^{n} \mathbf{X}_i (y_i - \pi_i) = \mathbf{X}'(\mathbf{y} - \boldsymbol{\pi})
$$

其中 $\boldsymbol{\pi} = (\pi_1, \pi_2, \ldots, \pi_n)'$。

**令梯度为 0：**

$$
\mathbf{X}'(\mathbf{y} - \boldsymbol{\pi}) = \mathbf{0}
$$

这是一个**非线性方程组**，没有解析解。

### 4.5 优化算法

**1. 梯度上升法（Gradient Ascent）**

$$
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} + \eta \frac{\partial \ell}{\partial \boldsymbol{\beta}}\bigg|_{\boldsymbol{\beta}^{(t)}}
$$

其中 $\eta$ 是学习率。

**2. Newton-Raphson 方法（推荐）**

**Hessian 矩阵（二阶导数）：**

$$
\mathbf{H} = \frac{\partial^2 \ell}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}'} = -\sum_{i=1}^{n} \pi_i(1 - \pi_i) \mathbf{X}_i \mathbf{X}_i' = -\mathbf{X}'\mathbf{W}\mathbf{X}
$$

其中 $\mathbf{W} = \text{diag}(\pi_i(1 - \pi_i))$ 是权重矩阵。

**Newton-Raphson 更新：**

$$
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \mathbf{H}^{-1} \frac{\partial \ell}{\partial \boldsymbol{\beta}}\bigg|_{\boldsymbol{\beta}^{(t)}}
$$

**迭代加权最小二乘（IRLS）：**

Newton-Raphson 方法等价于迭代求解加权最小二乘问题：

$$
\boldsymbol{\beta}^{(t+1)} = (\mathbf{X}'\mathbf{W}^{(t)}\mathbf{X})^{-1}\mathbf{X}'\mathbf{W}^{(t)}\mathbf{z}^{(t)}
$$

其中：
- $\mathbf{W}^{(t)} = \text{diag}(\pi_i^{(t)}(1 - \pi_i^{(t)}))$
- $\mathbf{z}^{(t)} = \mathbf{X}\boldsymbol{\beta}^{(t)} + \mathbf{W}^{(t)-1}(\mathbf{y} - \boldsymbol{\pi}^{(t)})$

### 4.6 估计量的性质

**渐近性质：**

1. **一致性**：$\hat{\boldsymbol{\beta}} \xrightarrow{p} \boldsymbol{\beta}$（当 $n \to \infty$）
2. **渐近正态性**：$\hat{\boldsymbol{\beta}} \sim \mathcal{N}(\boldsymbol{\beta}, (\mathbf{X}'\mathbf{W}\mathbf{X})^{-1})$（渐近）
3. **有效性**：在正则条件下，MLE 是渐近有效的

**协方差矩阵估计：**

$$
\widehat{\text{Var}}(\hat{\boldsymbol{\beta}}) = (\mathbf{X}'\hat{\mathbf{W}}\mathbf{X})^{-1}
$$

其中 $\hat{\mathbf{W}}$ 使用估计值 $\hat{\pi}_i$ 计算。

---

## 五、模型评估

### 5.1 混淆矩阵

**二分类混淆矩阵：**

| | 预测为 0 | 预测为 1 |
|---|---|---|
| **实际为 0** | TN (真阴性) | FP (假阳性) |
| **实际为 1** | FN (假阴性) | TP (真阳性) |

### 5.2 分类指标

**准确率（Accuracy）：**

$$
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
$$

**精确率（Precision）：**

$$
\text{Precision} = \frac{TP}{TP + FP}
$$

**召回率（Recall，灵敏度）：**

$$
\text{Recall} = \frac{TP}{TP + FN}
$$

**F1 分数：**

$$
F_1 = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
$$

**特异度（Specificity）：**

$$
\text{Specificity} = \frac{TN}{TN + FP}
$$

### 5.3 ROC 曲线和 AUC

**ROC 曲线（Receiver Operating Characteristic Curve）：**

- 横轴：假阳性率（FPR）= $\frac{FP}{FP + TN}$
- 纵轴：真阳性率（TPR）= $\frac{TP}{TP + FN}$ = Recall

**AUC（Area Under Curve）：**

- AUC 值在 $[0, 1]$ 之间
- AUC = 1：完美分类器
- AUC = 0.5：随机猜测
- AUC > 0.7：通常认为模型较好

### 5.4 对数似然和 AIC/BIC

**对数似然：**

$$
\ell(\hat{\boldsymbol{\beta}}) = \sum_{i=1}^{n} [y_i \ln \hat{\pi}_i + (1 - y_i) \ln(1 - \hat{\pi}_i)]
$$

**AIC（Akaike Information Criterion）：**

$$
\text{AIC} = -2\ell(\hat{\boldsymbol{\beta}}) + 2(p + 1)
$$

**BIC（Bayesian Information Criterion）：**

$$
\text{BIC} = -2\ell(\hat{\boldsymbol{\beta}}) + (p + 1)\ln(n)
$$

### 5.5 伪 R²

**McFadden's 伪 R²：**

$$
R^2_{\text{McFadden}} = 1 - \frac{\ell(\hat{\boldsymbol{\beta}})}{\ell(\hat{\boldsymbol{\beta}}_0)}
$$

其中 $\ell(\hat{\boldsymbol{\beta}}_0)$ 是只有截距项模型的对数似然。

**Cox & Snell 伪 R²：**

$$
R^2_{\text{Cox-Snell}} = 1 - \left(\frac{L(\hat{\boldsymbol{\beta}}_0)}{L(\hat{\boldsymbol{\beta}})}\right)^{2/n}
$$

**Nagelkerke 伪 R²：**

$$
R^2_{\text{Nagelkerke}} = \frac{R^2_{\text{Cox-Snell}}}{1 - L(\hat{\boldsymbol{\beta}}_0)^{2/n}}
$$

---

## 六、正则化逻辑回归

### 6.1 L2 正则化（Ridge）

**目标函数：**

$$
\min_{\boldsymbol{\beta}} \left\{ -\ell(\boldsymbol{\beta}) + \lambda\|\boldsymbol{\beta}\|_2^2 \right\}
$$

**特点：**
- 参数趋向于小值，但不为 0
- 防止过拟合
- 处理多重共线性

### 6.2 L1 正则化（Lasso）

**目标函数：**

$$
\min_{\boldsymbol{\beta}} \left\{ -\ell(\boldsymbol{\beta}) + \lambda\|\boldsymbol{\beta}\|_1 \right\}
$$

**特点：**
- 可以将参数压缩到 0
- 实现特征选择
- 产生稀疏模型

### 6.3 Elastic Net

**目标函数：**

$$
\min_{\boldsymbol{\beta}} \left\{ -\ell(\boldsymbol{\beta}) + \lambda\left[\alpha\|\boldsymbol{\beta}\|_1 + (1-\alpha)\frac{1}{2}\|\boldsymbol{\beta}\|_2^2\right] \right\}
$$

**特点：**
- 结合 L1 和 L2 正则化
- 兼具两种方法的优点

---

## 七、逻辑回归的假设与诊断

### 7.1 基本假设

**1. 线性关系**
- 对数几率与特征之间存在线性关系
- 可以通过添加交互项或多项式项扩展

**2. 独立性**
- 观测值之间相互独立
- 对于时间序列数据，可能不满足

**3. 无多重共线性**
- 特征之间不应该高度相关
- 可以使用正则化处理

### 7.2 模型诊断

**1. 残差分析**

**Pearson 残差：**

$$
r_i = \frac{y_i - \hat{\pi}_i}{\sqrt{\hat{\pi}_i(1 - \hat{\pi}_i)}}
$$

**Deviance 残差：**

$$
d_i = \text{sign}(y_i - \hat{\pi}_i) \sqrt{-2[y_i \ln \hat{\pi}_i + (1 - y_i) \ln(1 - \hat{\pi}_i)]}
$$

**2. 影响点检测**

**Cook's Distance：**

$$
D_i = \frac{(r_i^*)^2}{p + 1} \cdot \frac{h_i}{1 - h_i}
$$

其中 $h_i$ 是杠杆值。

**3. 多重共线性检测**

使用方差膨胀因子（VIF）检测多重共线性。

---

## 八、在量化交易中的应用

### 8.1 涨跌预测

**应用场景：**

使用逻辑回归预测股价涨跌：

**模型：**

$$
P(\text{上涨} | \mathbf{X}) = \sigma(\beta_0 + \beta_1 \text{RSI} + \beta_2 \text{MACD} + \cdots)
$$

**特征：**
- 技术指标：RSI、MACD、均线等
- 价格特征：收益率、波动率等
- 市场特征：成交量、换手率等

**优势：**
- 输出概率，便于风险管理
- 可解释性强
- 计算高效

### 8.2 交易信号生成

**应用场景：**

根据预测概率生成交易信号：

**策略：**
- 如果 $P(\text{上涨}) > 0.6$：买入信号
- 如果 $P(\text{上涨}) < 0.4$：卖出信号
- 否则：持有

**优势：**
- 可以设置不同的概率阈值
- 根据风险偏好调整策略

### 8.3 风险管理

**应用场景：**

使用逻辑回归进行风险分类：

**模型：**

$$
P(\text{高风险} | \mathbf{X}) = \sigma(\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p)
$$

**特征：**
- 资产配置比例
- 行业分布
- 相关性指标
- 波动率指标

### 8.4 信用评分

**应用场景：**

在量化交易中评估交易对手的信用风险：

**模型：**

$$
P(\text{违约} | \mathbf{X}) = \sigma(\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p)
$$

**特征：**
- 财务指标
- 历史表现
- 市场指标

### 8.5 实际应用注意事项

**1. 类别不平衡**
- 使用类别权重
- 使用采样方法（过采样、欠采样）
- 调整决策阈值

**2. 时间序列特性**
- 使用时间序列交叉验证
- 避免数据泄露
- 注意时间依赖性

**3. 特征工程**
- 创建有意义的特征
- 处理缺失值
- 特征选择

**4. 模型验证**
- 使用样本外测试
- 监控模型性能
- 定期重新训练

---

## 九、Python 实现示例

### 9.1 基础逻辑回归

```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import numpy as np
import pandas as pd

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 逻辑回归
lr = LogisticRegression(
    penalty='none',  # 无正则化
    solver='lbfgs',  # 优化算法
    max_iter=1000,
    random_state=42
)

lr.fit(X_train, y_train)

# 预测
y_pred = lr.predict(X_test)
y_proba = lr.predict_proba(X_test)[:, 1]  # 预测概率

# 评估
accuracy = accuracy_score(y_test, y_pred)
print(f"准确率: {accuracy:.4f}")

print("\n分类报告:")
print(classification_report(y_test, y_pred))

print("\n混淆矩阵:")
print(confusion_matrix(y_test, y_pred))
```

### 9.2 正则化逻辑回归

```python
from sklearn.linear_model import LogisticRegressionCV

# L2 正则化（Ridge）
lr_ridge = LogisticRegressionCV(
    Cs=10,  # 正则化参数的范围
    penalty='l2',
    cv=5,  # 5 折交叉验证
    solver='lbfgs',
    max_iter=1000,
    random_state=42
)

lr_ridge.fit(X_train, y_train)
print(f"最优 C (1/lambda): {lr_ridge.C_[0]:.4f}")

# L1 正则化（Lasso）
lr_lasso = LogisticRegressionCV(
    Cs=10,
    penalty='l1',
    cv=5,
    solver='liblinear',  # L1 需要使用 liblinear
    max_iter=1000,
    random_state=42
)

lr_lasso.fit(X_train, y_train)
print(f"最优 C: {lr_lasso.C_[0]:.4f}")
print(f"选择的特征数: {np.sum(lr_lasso.coef_[0] != 0)}")
```

### 9.3 ROC 曲线和 AUC

```python
from sklearn.metrics import roc_curve, auc, roc_auc_score
import matplotlib.pyplot as plt

# 计算 ROC 曲线
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
roc_auc = auc(fpr, tpr)

# 绘制 ROC 曲线
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, 
         label=f'ROC 曲线 (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='随机猜测')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('假阳性率 (FPR)', fontsize=12)
plt.ylabel('真阳性率 (TPR)', fontsize=12)
plt.title('ROC 曲线', fontsize=14)
plt.legend(loc="lower right")
plt.grid(True, alpha=0.3)
plt.show()

print(f"AUC: {roc_auc:.4f}")
```

### 9.4 系数解释

```python
# 系数和几率比
coefficients = pd.DataFrame({
    'feature': feature_names,
    'coefficient': lr.coef_[0],
    'odds_ratio': np.exp(lr.coef_[0])
}).sort_values('coefficient', key=abs, ascending=False)

print("系数和几率比:")
print(coefficients)

# 可视化系数
plt.figure(figsize=(10, 6))
plt.barh(range(len(coefficients)), coefficients['coefficient'])
plt.yticks(range(len(coefficients)), coefficients['feature'])
plt.xlabel('系数', fontsize=12)
plt.title('逻辑回归系数', fontsize=14)
plt.axvline(x=0, color='r', linestyle='--', alpha=0.5)
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()
```

### 9.5 量化交易应用示例

```python
import yfinance as yf
import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegressionCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

def get_stock_data(symbol, period='3y'):
    """获取股票数据"""
    stock = yf.Ticker(symbol)
    data = stock.history(period=period)
    return data

def calculate_features(data):
    """计算技术指标特征"""
    df = data.copy()
    
    # 收益率
    df['Return'] = df['Close'].pct_change()
    
    # RSI
    delta = df['Close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df['RSI'] = 100 - (100 / (1 + rs))
    
    # MACD
    exp1 = df['Close'].ewm(span=12, adjust=False).mean()
    exp2 = df['Close'].ewm(span=26, adjust=False).mean()
    df['MACD'] = exp1 - exp2
    df['MACD_signal'] = df['MACD'].ewm(span=9, adjust=False).mean()
    df['MACD_diff'] = df['MACD'] - df['MACD_signal']
    
    # 均线
    df['MA_5'] = df['Close'].rolling(window=5).mean()
    df['MA_20'] = df['Close'].rolling(window=20).mean()
    df['MA_50'] = df['Close'].rolling(window=50).mean()
    df['MA_diff_5_20'] = (df['MA_5'] - df['MA_20']) / df['Close']
    df['MA_diff_20_50'] = (df['MA_20'] - df['MA_50']) / df['Close']
    
    # 波动率
    df['Volatility_5'] = df['Return'].rolling(window=5).std()
    df['Volatility_20'] = df['Return'].rolling(window=20).std()
    
    # 成交量
    df['Volume_MA'] = df['Volume'].rolling(window=20).mean()
    df['Volume_Ratio'] = df['Volume'] / df['Volume_MA']
    
    # 价格位置
    df['High_Low_Ratio'] = (df['High'] - df['Low']) / df['Close']
    df['Close_MA_Ratio'] = df['Close'] / df['MA_20']
    
    return df

def create_target(data, horizon=5):
    """创建目标变量（未来是否上涨）"""
    data['Future_Return'] = data['Close'].pct_change(horizon).shift(-horizon)
    data['Target'] = (data['Future_Return'] > 0).astype(int)
    return data

def main():
    # 获取数据
    data = get_stock_data('AAPL', period='3y')
    
    # 计算特征
    data = calculate_features(data)
    
    # 创建目标变量
    data = create_target(data, horizon=5)
    
    # 选择特征
    feature_cols = [
        'RSI', 'MACD', 'MACD_diff',
        'MA_diff_5_20', 'MA_diff_20_50',
        'Volatility_5', 'Volatility_20',
        'Volume_Ratio',
        'High_Low_Ratio', 'Close_MA_Ratio'
    ]
    
    # 删除缺失值
    data = data[feature_cols + ['Target']].dropna()
    
    X = data[feature_cols].values
    y = data['Target'].values
    
    print(f"样本数: {len(X)}")
    print(f"正样本比例: {y.mean():.2%}")
    
    # 时间序列划分
    split_idx = int(len(X) * 0.8)
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    # 标准化（逻辑回归通常不需要，但有助于正则化）
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # 逻辑回归（L2 正则化）
    print("\n" + "=" * 60)
    print("逻辑回归 - 涨跌预测")
    print("=" * 60)
    
    lr = LogisticRegressionCV(
        Cs=10,
        penalty='l2',
        cv=5,
        solver='lbfgs',
        max_iter=1000,
        random_state=42,
        class_weight='balanced'  # 处理类别不平衡
    )
    
    lr.fit(X_train_scaled, y_train)
    
    # 预测
    y_pred = lr.predict(X_test_scaled)
    y_proba = lr.predict_proba(X_test_scaled)[:, 1]
    
    # 评估
    accuracy = accuracy_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_proba)
    
    print(f"最优 C: {lr.C_[0]:.4f}")
    print(f"测试集准确率: {accuracy:.4f}")
    print(f"测试集 AUC: {auc:.4f}")
    
    print("\n分类报告:")
    print(classification_report(y_test, y_pred))
    
    # 系数和几率比
    coefficients = pd.DataFrame({
        'feature': feature_cols,
        'coefficient': lr.coef_[0],
        'odds_ratio': np.exp(lr.coef_[0])
    }).sort_values('coefficient', key=abs, ascending=False)
    
    print("\n系数和几率比:")
    print(coefficients)
    
    return lr, data, coefficients

if __name__ == '__main__':
    model, data, coefs = main()
```

### 9.6 多分类逻辑回归

```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# 多分类逻辑回归
lr_multiclass = LogisticRegression(
    multi_class='multinomial',  # 多分类
    solver='lbfgs',
    max_iter=1000,
    random_state=42
)

lr_multiclass.fit(X_train, y_train)

# 预测
y_pred = lr_multiclass.predict(X_test)
y_proba = lr_multiclass.predict_proba(X_test)

# 评估
accuracy = accuracy_score(y_test, y_pred)
print(f"准确率: {accuracy:.4f}")

print("\n分类报告:")
print(classification_report(y_test, y_pred))
```

---

## 十、总结

### 10.1 核心要点

**1. 逻辑回归的本质**
- 虽然名字是"回归"，但实际上是分类算法
- 使用逻辑函数将线性组合映射到 $[0, 1]$ 区间
- 输出可以解释为概率

**2. 核心优势**
- 输出概率，便于解释和决策
- 可解释性强，系数有明确含义
- 计算高效，训练和预测速度快
- 理论基础完备

**3. 参数估计**
- 使用最大似然估计
- 通过 Newton-Raphson 方法或梯度上升法求解
- 没有解析解，需要迭代优化

### 10.2 优势与局限

**优势：**
- ✅ 输出概率
- ✅ 可解释性强
- ✅ 计算高效
- ✅ 不需要特征缩放
- ✅ 理论基础完备

**局限：**
- ❌ 假设线性关系（对数几率）
- ❌ 对异常值敏感
- ❌ 需要足够样本量
- ❌ 可能过拟合（高维数据）

### 10.3 在量化交易中的应用

**主要应用：**
1. **涨跌预测**：预测股价涨跌方向
2. **交易信号生成**：根据概率生成交易信号
3. **风险管理**：风险分类和评估
4. **信用评分**：评估交易对手信用风险

**注意事项：**
- 处理类别不平衡
- 使用时间序列交叉验证
- 注意特征工程
- 监控模型性能

### 10.4 学习建议

**1. 理论基础**
- 理解逻辑函数和 Sigmoid 函数
- 理解最大似然估计
- 理解几率和对数几率

**2. 实践应用**
- 使用 scikit-learn 实现逻辑回归
- 进行模型评估和诊断
- 在量化交易中应用逻辑回归

**3. 进阶学习**
- 学习正则化逻辑回归
- 学习多分类逻辑回归
- 学习其他分类算法（SVM、随机森林等）

---

**总结：逻辑回归是一种重要的分类算法，通过逻辑函数将线性组合映射到概率空间，输出可以解释为概率。在量化交易中，逻辑回归被广泛应用于涨跌预测、交易信号生成和风险管理。理解逻辑回归的原理、估计和应用，对于构建有效的量化模型至关重要。通过合理使用正则化、处理类别不平衡等方法，可以提高模型的性能和泛化能力。**

