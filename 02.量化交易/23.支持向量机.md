# 支持向量机（Support Vector Machine, SVM）：原理、算法与应用

## 目录
1. [支持向量机概述](#一-支持向量机概述)
2. [线性支持向量机](#二-线性支持向量机)
3. [软间隔支持向量机](#三-软间隔支持向量机)
4. [非线性支持向量机：核方法](#四-非线性支持向量机核方法)
5. [支持向量回归（SVR）](#五-支持向量回归svr)
6. [多分类支持向量机](#六-多分类支持向量机)
7. [在量化交易中的应用](#七-在量化交易中的应用)
8. [Python 实现示例](#八-python-实现示例)
9. [总结](#九-总结)

---

## 一、支持向量机概述

### 1.1 什么是支持向量机

**支持向量机（Support Vector Machine, SVM）**是一种基于统计学习理论的监督学习算法，主要用于分类和回归问题。

**核心思想：**
- 寻找最优的**分离超平面**，使得两类样本之间的**间隔（Margin）**最大
- 利用**支持向量**（Support Vectors）定义决策边界
- 通过**核技巧**处理非线性问题

### 1.2 SVM 的发展历史

- **1960s**：Vapnik 和 Chervonenkis 提出统计学习理论
- **1990s**：Boser、Guyon 和 Vapnik 提出支持向量机
- **1995**：Cortes 和 Vapnik 提出软间隔 SVM
- **1990s-2000s**：核方法的发展，使 SVM 能够处理非线性问题

### 1.3 SVM 的主要类型

**1. 线性支持向量机（Linear SVM）**
- 硬间隔：数据线性可分
- 软间隔：数据近似线性可分

**2. 非线性支持向量机（Nonlinear SVM）**
- 使用核技巧将数据映射到高维空间
- 在高维空间中应用线性 SVM

**3. 支持向量回归（SVR）**
- 用于回归问题
- 使用 ε-不敏感损失函数

### 1.4 SVM 的优势

**主要优势：**
- ✅ **理论基础完备**：基于统计学习理论和结构风险最小化
- ✅ **泛化能力强**：通过最大化间隔提高泛化能力
- ✅ **处理非线性问题**：通过核技巧处理复杂的非线性关系
- ✅ **稀疏解**：只使用支持向量，模型简洁
- ✅ **对高维数据有效**：在高维空间中表现良好
- ✅ **内存效率高**：只需要存储支持向量

### 1.5 SVM 的局限性

**主要局限：**
- ❌ **计算复杂度高**：训练时间复杂度为 $O(n^2)$ 或 $O(n^3)$
- ❌ **对大规模数据不友好**：样本数很大时训练慢
- ❌ **需要调参**：核函数选择和参数调优
- ❌ **对特征缩放敏感**：需要标准化特征
- ❌ **可解释性较差**：特别是使用核函数时

### 1.6 SVM 的应用领域

**主要应用：**
1. **文本分类**：垃圾邮件检测、情感分析
2. **图像识别**：手写识别、人脸识别
3. **生物信息学**：基因分类、蛋白质结构预测
4. **金融分析**：信用评分、欺诈检测
5. **量化交易**：涨跌预测、交易信号生成

---

## 二、线性支持向量机

### 2.1 问题设置

**二分类问题：**

给定训练数据集：
$$
D = \{(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \ldots, (\mathbf{x}_n, y_n)\}
$$

其中：
- $\mathbf{x}_i \in \mathbb{R}^p$：特征向量
- $y_i \in \{-1, +1\}$：类别标签

**目标：** 找到一个超平面 $f(\mathbf{x}) = \mathbf{w}' \mathbf{x} + b = 0$，将两类样本分开。

### 2.2 分离超平面

**超平面方程：**

$$
f(\mathbf{x}) = \mathbf{w}' \mathbf{x} + b = 0
$$

其中：
- $\mathbf{w} \in \mathbb{R}^p$：法向量（权重向量）
- $b \in \mathbb{R}$：偏置项

**分类规则：**

$$
\hat{y} = \begin{cases}
+1 & \text{if } f(\mathbf{x}) = \mathbf{w}' \mathbf{x} + b \geq 0 \\
-1 & \text{if } f(\mathbf{x}) = \mathbf{w}' \mathbf{x} + b < 0
\end{cases}
$$

### 2.3 函数间隔和几何间隔

**函数间隔（Functional Margin）：**

对于样本 $(\mathbf{x}_i, y_i)$，函数间隔定义为：

$$
\hat{\gamma}_i = y_i(\mathbf{w}' \mathbf{x}_i + b)
$$

**几何间隔（Geometric Margin）：**

几何间隔是样本到超平面的距离：

$$
\gamma_i = \frac{y_i(\mathbf{w}' \mathbf{x}_i + b)}{\|\mathbf{w}\|} = \frac{\hat{\gamma}_i}{\|\mathbf{w}\|}
$$

**数据集的最小间隔：**

$$
\gamma = \min_{i=1,\ldots,n} \gamma_i
$$

### 2.4 最大间隔分类器

**目标：** 找到使几何间隔最大的超平面。

**优化问题：**

$$
\begin{aligned}
\max_{\mathbf{w}, b} \quad & \gamma \\
\text{s.t.} \quad & \frac{y_i(\mathbf{w}' \mathbf{x}_i + b)}{\|\mathbf{w}\|} \geq \gamma, \quad i = 1, \ldots, n
\end{aligned}
$$

**等价形式：**

由于可以任意缩放 $\mathbf{w}$ 和 $b$，可以固定函数间隔为 1，问题转化为：

$$
\begin{aligned}
\min_{\mathbf{w}, b} \quad & \frac{1}{2}\|\mathbf{w}\|^2 \\
\text{s.t.} \quad & y_i(\mathbf{w}' \mathbf{x}_i + b) \geq 1, \quad i = 1, \ldots, n
\end{aligned}
$$

这是一个**凸二次规划问题**。

### 2.5 拉格朗日对偶

**原始问题（Primal Problem）：**

$$
\begin{aligned}
\min_{\mathbf{w}, b} \quad & \frac{1}{2}\|\mathbf{w}\|^2 \\
\text{s.t.} \quad & y_i(\mathbf{w}' \mathbf{x}_i + b) - 1 \geq 0, \quad i = 1, \ldots, n
\end{aligned}
$$

**拉格朗日函数：**

$$
L(\mathbf{w}, b, \boldsymbol{\alpha}) = \frac{1}{2}\|\mathbf{w}\|^2 - \sum_{i=1}^{n} \alpha_i [y_i(\mathbf{w}' \mathbf{x}_i + b) - 1]
$$

其中 $\alpha_i \geq 0$ 是拉格朗日乘数。

**对偶问题（Dual Problem）：**

$$
\begin{aligned}
\max_{\boldsymbol{\alpha}} \quad & \sum_{i=1}^{n} \alpha_i - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_i \alpha_j y_i y_j \mathbf{x}_i' \mathbf{x}_j \\
\text{s.t.} \quad & \sum_{i=1}^{n} \alpha_i y_i = 0 \\
& \alpha_i \geq 0, \quad i = 1, \ldots, n
\end{aligned}
$$

### 2.6 KKT 条件

**KKT（Karush-Kuhn-Tucker）条件：**

1. **原始可行性**：
   $$
   y_i(\mathbf{w}' \mathbf{x}_i + b) - 1 \geq 0, \quad i = 1, \ldots, n
   $$

2. **对偶可行性**：
   $$
   \alpha_i \geq 0, \quad i = 1, \ldots, n
   $$

3. **互补松弛性**：
   $$
   \alpha_i [y_i(\mathbf{w}' \mathbf{x}_i + b) - 1] = 0, \quad i = 1, \ldots, n
   $$

4. **梯度条件**：
   $$
   \frac{\partial L}{\partial \mathbf{w}} = \mathbf{w} - \sum_{i=1}^{n} \alpha_i y_i \mathbf{x}_i = 0
   $$
   $$
   \frac{\partial L}{\partial b} = -\sum_{i=1}^{n} \alpha_i y_i = 0
   $$

### 2.7 支持向量

**支持向量的定义：**

根据互补松弛性，如果 $\alpha_i > 0$，则：

$$
y_i(\mathbf{w}' \mathbf{x}_i + b) - 1 = 0
$$

即样本 $\mathbf{x}_i$ 位于间隔边界上，这样的样本称为**支持向量（Support Vectors）**。

**权重向量的表示：**

从梯度条件得到：

$$
\mathbf{w} = \sum_{i=1}^{n} \alpha_i y_i \mathbf{x}_i = \sum_{i \in SV} \alpha_i y_i \mathbf{x}_i
$$

其中 $SV$ 是支持向量的集合。**权重向量只由支持向量决定**。

**偏置项的求解：**

对于任意支持向量 $\mathbf{x}_s$：

$$
b = y_s - \mathbf{w}' \mathbf{x}_s = y_s - \sum_{i \in SV} \alpha_i y_i \mathbf{x}_i' \mathbf{x}_s
$$

### 2.8 决策函数

**决策函数：**

$$
f(\mathbf{x}) = \mathbf{w}' \mathbf{x} + b = \sum_{i \in SV} \alpha_i y_i \mathbf{x}_i' \mathbf{x} + b
$$

**分类规则：**

$$
\hat{y} = \text{sign}(f(\mathbf{x}))
$$

### 2.9 硬间隔 SVM 的优缺点

**优点：**
- ✅ 理论完备，有最优解
- ✅ 只使用支持向量，模型简洁
- ✅ 泛化能力强

**缺点：**
- ❌ 要求数据严格线性可分
- ❌ 对噪声和异常值敏感
- ❌ 实际应用中很少满足线性可分条件

---

## 三、软间隔支持向量机

### 3.1 软间隔的动机

**硬间隔的问题：**
- 要求数据严格线性可分
- 对噪声和异常值敏感
- 可能过拟合

**软间隔的解决方案：**
- 允许一些样本违反间隔约束
- 引入**松弛变量（Slack Variables）**
- 在最大化间隔和最小化分类错误之间平衡

### 3.2 软间隔优化问题

**原始问题：**

$$
\begin{aligned}
\min_{\mathbf{w}, b, \boldsymbol{\xi}} \quad & \frac{1}{2}\|\mathbf{w}\|^2 + C \sum_{i=1}^{n} \xi_i \\
\text{s.t.} \quad & y_i(\mathbf{w}' \mathbf{x}_i + b) \geq 1 - \xi_i, \quad i = 1, \ldots, n \\
& \xi_i \geq 0, \quad i = 1, \ldots, n
\end{aligned}
$$

其中：
- $\xi_i$：松弛变量，表示样本 $\mathbf{x}_i$ 违反间隔约束的程度
- $C > 0$：惩罚参数，控制对误分类的惩罚程度

**参数 $C$ 的作用：**
- **$C$ 很大**：对误分类惩罚大，间隔小，可能过拟合
- **$C$ 很小**：对误分类惩罚小，间隔大，可能欠拟合

### 3.3 对偶问题

**拉格朗日函数：**

$$
L(\mathbf{w}, b, \boldsymbol{\xi}, \boldsymbol{\alpha}, \boldsymbol{\mu}) = \frac{1}{2}\|\mathbf{w}\|^2 + C \sum_{i=1}^{n} \xi_i - \sum_{i=1}^{n} \alpha_i [y_i(\mathbf{w}' \mathbf{x}_i + b) - 1 + \xi_i] - \sum_{i=1}^{n} \mu_i \xi_i
$$

**对偶问题：**

$$
\begin{aligned}
\max_{\boldsymbol{\alpha}} \quad & \sum_{i=1}^{n} \alpha_i - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_i \alpha_j y_i y_j \mathbf{x}_i' \mathbf{x}_j \\
\text{s.t.} \quad & \sum_{i=1}^{n} \alpha_i y_i = 0 \\
& 0 \leq \alpha_i \leq C, \quad i = 1, \ldots, n
\end{aligned}
$$

与硬间隔的区别：$\alpha_i$ 的上界从 $\infty$ 变为 $C$。

### 3.4 支持向量的类型

**根据 $\alpha_i$ 的值，支持向量分为三类：**

1. **间隔支持向量**（$0 < \alpha_i < C$）：
   - 位于间隔边界上
   - $\xi_i = 0$，没有违反约束

2. **误分类支持向量**（$\alpha_i = C, \xi_i > 0$）：
   - 违反间隔约束
   - 位于间隔内或错误一侧

3. **非支持向量**（$\alpha_i = 0$）：
   - 位于正确一侧，远离间隔边界
   - 对决策函数没有贡献

### 3.5 损失函数视角

**软间隔 SVM 等价于最小化以下损失函数：**

$$
\min_{\mathbf{w}, b} \quad \frac{1}{2}\|\mathbf{w}\|^2 + C \sum_{i=1}^{n} \max(0, 1 - y_i(\mathbf{w}' \mathbf{x}_i + b))
$$

其中 $\max(0, 1 - y_i(\mathbf{w}' \mathbf{x}_i + b))$ 是**合页损失（Hinge Loss）**。

**合页损失函数：**

$$
L_{\text{hinge}}(y, f(\mathbf{x})) = \max(0, 1 - y f(\mathbf{x}))
$$

### 3.6 参数 $C$ 的选择

**选择方法：**

1. **交叉验证**：在验证集上评估不同 $C$ 值的性能
2. **网格搜索**：在 $C \in \{10^{-3}, 10^{-2}, \ldots, 10^{3}\}$ 上搜索
3. **经验规则**：从 $C = 1$ 开始，根据结果调整

---

## 四、非线性支持向量机：核方法

### 4.1 核方法的动机

**线性 SVM 的局限性：**
- 只能处理线性可分的数据
- 对于非线性可分的数据，需要非线性决策边界

**核方法的解决方案：**
- 将数据映射到高维特征空间
- 在高维空间中应用线性 SVM
- 通过核函数隐式计算高维空间的内积

### 4.2 特征映射

**特征映射函数：**

$$
\phi: \mathbb{R}^p \rightarrow \mathcal{H}
$$

将原始特征空间 $\mathbb{R}^p$ 映射到高维特征空间 $\mathcal{H}$。

**高维空间中的 SVM：**

在高维空间中，决策函数为：

$$
f(\mathbf{x}) = \sum_{i \in SV} \alpha_i y_i \phi(\mathbf{x}_i)' \phi(\mathbf{x}) + b
$$

### 4.3 核函数

**核函数（Kernel Function）：**

$$
\kappa(\mathbf{x}_i, \mathbf{x}_j) = \phi(\mathbf{x}_i)' \phi(\mathbf{x}_j)
$$

核函数直接计算高维空间中的内积，**不需要显式计算映射函数**。

**决策函数（使用核函数）：**

$$
f(\mathbf{x}) = \sum_{i \in SV} \alpha_i y_i \kappa(\mathbf{x}_i, \mathbf{x}) + b
$$

**对偶问题（使用核函数）：**

$$
\begin{aligned}
\max_{\boldsymbol{\alpha}} \quad & \sum_{i=1}^{n} \alpha_i - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_i \alpha_j y_i y_j \kappa(\mathbf{x}_i, \mathbf{x}_j) \\
\text{s.t.} \quad & \sum_{i=1}^{n} \alpha_i y_i = 0 \\
& 0 \leq \alpha_i \leq C, \quad i = 1, \ldots, n
\end{aligned}
$$

### 4.4 常用核函数

**1. 线性核（Linear Kernel）：**

$$
\kappa(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{x}_i' \mathbf{x}_j
$$

等价于线性 SVM。

**2. 多项式核（Polynomial Kernel）：**

$$
\kappa(\mathbf{x}_i, \mathbf{x}_j) = (\gamma \mathbf{x}_i' \mathbf{x}_j + r)^d
$$

其中：
- $d$：多项式次数
- $\gamma$：缩放参数
- $r$：偏置参数

**3. RBF 核（Radial Basis Function Kernel，高斯核）：**

$$
\kappa(\mathbf{x}_i, \mathbf{x}_j) = \exp(-\gamma \|\mathbf{x}_i - \mathbf{x}_j\|^2)
$$

其中 $\gamma > 0$ 是带宽参数。

**4. Sigmoid 核：**

$$
\kappa(\mathbf{x}_i, \mathbf{x}_j) = \tanh(\gamma \mathbf{x}_i' \mathbf{x}_j + r)
$$

### 4.5 核函数的性质

**Mercer 条件：**

核函数必须满足 Mercer 条件（正定核），即对于任意有限样本集，核矩阵 $K_{ij} = \kappa(\mathbf{x}_i, \mathbf{x}_j)$ 是半正定的。

**常用核函数都满足 Mercer 条件。**

### 4.6 RBF 核的参数选择

**RBF 核的参数 $\gamma$：**

- **$\gamma$ 很大**：决策边界复杂，可能过拟合
- **$\gamma$ 很小**：决策边界平滑，可能欠拟合

**选择方法：**
- 交叉验证
- 网格搜索：$\gamma \in \{10^{-3}, 10^{-2}, \ldots, 10^{3}\}$

**参数组合搜索：**

同时搜索 $C$ 和 $\gamma$：

```python
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': [0.001, 0.01, 0.1, 1, 10]
}
```

### 4.7 核技巧的优势

**优势：**
- ✅ 可以处理非线性问题
- ✅ 不需要显式计算高维映射
- ✅ 计算复杂度与特征维度无关（只与样本数有关）

**缺点：**
- ❌ 需要选择合适的核函数和参数
- ❌ 可解释性较差
- ❌ 对大规模数据计算量大

---

## 五、支持向量回归（SVR）

### 5.1 支持向量回归概述

**支持向量回归（Support Vector Regression, SVR）**是 SVM 在回归问题上的扩展。

**核心思想：**
- 寻找一个函数 $f(\mathbf{x}) = \mathbf{w}' \mathbf{x} + b$，使得大部分样本落在 $\varepsilon$-带内
- 使用 $\varepsilon$-不敏感损失函数
- 只惩罚落在 $\varepsilon$-带外的样本

### 5.2 ε-不敏感损失函数

**ε-不敏感损失：**

$$
L_\varepsilon(y, f(\mathbf{x})) = \begin{cases}
0 & \text{if } |y - f(\mathbf{x})| \leq \varepsilon \\
|y - f(\mathbf{x})| - \varepsilon & \text{otherwise}
\end{cases}
$$

**特点：**
- 如果预测误差在 $\varepsilon$ 以内，损失为 0
- 只惩罚误差超过 $\varepsilon$ 的样本

### 5.3 SVR 优化问题

**原始问题：**

$$
\begin{aligned}
\min_{\mathbf{w}, b, \boldsymbol{\xi}, \boldsymbol{\xi}^*} \quad & \frac{1}{2}\|\mathbf{w}\|^2 + C \sum_{i=1}^{n} (\xi_i + \xi_i^*) \\
\text{s.t.} \quad & y_i - (\mathbf{w}' \mathbf{x}_i + b) \leq \varepsilon + \xi_i, \quad i = 1, \ldots, n \\
& (\mathbf{w}' \mathbf{x}_i + b) - y_i \leq \varepsilon + \xi_i^*, \quad i = 1, \ldots, n \\
& \xi_i, \xi_i^* \geq 0, \quad i = 1, \ldots, n
\end{aligned}
$$

其中：
- $\xi_i$：上界违反量
- $\xi_i^*$：下界违反量
- $C$：惩罚参数
- $\varepsilon$：不敏感参数

### 5.4 SVR 对偶问题

**对偶问题：**

$$
\begin{aligned}
\max_{\boldsymbol{\alpha}, \boldsymbol{\alpha}^*} \quad & -\frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} (\alpha_i - \alpha_i^*)(\alpha_j - \alpha_j^*) \kappa(\mathbf{x}_i, \mathbf{x}_j) \\
& \quad - \varepsilon \sum_{i=1}^{n} (\alpha_i + \alpha_i^*) + \sum_{i=1}^{n} y_i (\alpha_i - \alpha_i^*) \\
\text{s.t.} \quad & \sum_{i=1}^{n} (\alpha_i - \alpha_i^*) = 0 \\
& 0 \leq \alpha_i, \alpha_i^* \leq C, \quad i = 1, \ldots, n
\end{aligned}
$$

### 5.5 SVR 决策函数

**决策函数：**

$$
f(\mathbf{x}) = \sum_{i=1}^{n} (\alpha_i - \alpha_i^*) \kappa(\mathbf{x}_i, \mathbf{x}) + b
$$

**支持向量：**

满足 $|\alpha_i - \alpha_i^*| > 0$ 的样本是支持向量。

---

## 六、多分类支持向量机

### 6.1 多分类问题

**问题设置：**

$K$ 个类别：$y_i \in \{1, 2, \ldots, K\}$

**SVM 本质上是二分类方法，需要扩展来处理多分类问题。**

### 6.2 一对多（One-vs-Rest, OvR）

**方法：**

训练 $K$ 个二分类 SVM：
- 第 $k$ 个 SVM：类别 $k$ vs 其他所有类别

**分类规则：**

$$
\hat{y} = \arg\max_{k=1,\ldots,K} f_k(\mathbf{x})
$$

其中 $f_k(\mathbf{x})$ 是第 $k$ 个 SVM 的决策函数值。

**优点：**
- 简单直观
- 只需要训练 $K$ 个分类器

**缺点：**
- 类别不平衡（一个类别 vs 多个类别）
- 可能产生多个分类器都预测为正的情况

### 6.3 一对一（One-vs-One, OvO）

**方法：**

训练 $\frac{K(K-1)}{2}$ 个二分类 SVM：
- 每对类别训练一个 SVM

**分类规则：**

使用投票机制：每个 SVM 投票，得票最多的类别获胜。

**优点：**
- 每个分类器只处理两个类别，更平衡
- 通常比 OvR 更准确

**缺点：**
- 需要训练更多分类器
- 预测时需要所有分类器投票

### 6.4 有向无环图（DAG-SVM）

**方法：**

使用有向无环图组织一对一分类器，减少预测时的计算量。

---

## 七、在量化交易中的应用

### 7.1 涨跌预测

**使用 SVM 预测股价涨跌：**

**特征：**
- 技术指标：RSI、MACD、均线等
- 价格特征：收益率、波动率等
- 市场特征：成交量、换手率等

**目标：**
- 预测未来 N 天的涨跌方向（上涨/下跌）

**方法选择：**
- **线性 SVM**：如果特征与涨跌的关系近似线性
- **RBF 核 SVM**：如果关系复杂非线性

**优势：**
- 泛化能力强
- 对高维特征有效
- 可以处理非线性关系

### 7.2 交易信号生成

**使用 SVM 生成交易信号：**

**策略：**
- 如果 $f(\mathbf{x}) > \theta$：买入信号
- 如果 $f(\mathbf{x}) < -\theta$：卖出信号
- 否则：持有

其中 $\theta$ 是阈值，可以根据风险偏好调整。

### 7.3 市场状态识别

**使用 SVM 识别市场状态：**

**市场状态：**
- 牛市：持续上涨趋势
- 熊市：持续下跌趋势
- 震荡市：横盘整理

**方法：**
- 使用多分类 SVM（OvO 或 OvR）
- 或使用多个二分类 SVM

### 7.4 特征选择

**SVM 的特征重要性：**

- 对于线性 SVM，可以通过权重向量 $\mathbf{w}$ 的绝对值判断特征重要性
- 对于非线性 SVM，特征重要性分析较困难

### 7.5 实际应用注意事项

**1. 数据预处理**
- **标准化**：SVM 对特征尺度敏感，必须标准化
- **处理缺失值**：删除或插补
- **处理异常值**：识别和处理异常值

**2. 参数调优**
- **$C$ 参数**：控制对误分类的惩罚
- **核函数选择**：线性、RBF、多项式等
- **核参数**：如 RBF 核的 $\gamma$
- 使用交叉验证选择最优参数

**3. 模型验证**
- 使用时间序列交叉验证（避免数据泄露）
- 监控模型性能
- 定期重新训练

**4. 计算效率**
- 对于大规模数据，考虑使用线性 SVM 或近似方法
- 使用增量学习或在线学习

---

## 八、Python 实现示例

### 8.1 线性 SVM

```python
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import numpy as np
import pandas as pd

# 准备数据
# X: 特征矩阵 (n_samples, n_features)
# y: 标签 (n_samples,)

# 标准化（SVM 对尺度敏感）
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

# 创建线性 SVM
svm_linear = SVC(
    kernel='linear',
    C=1.0,  # 惩罚参数
    random_state=42
)

# 训练模型
svm_linear.fit(X_train, y_train)

# 预测
y_pred = svm_linear.predict(X_test)
y_decision = svm_linear.decision_function(X_test)  # 决策函数值

# 评估
accuracy = accuracy_score(y_test, y_pred)
print(f"准确率: {accuracy:.4f}")

print("\n分类报告:")
print(classification_report(y_test, y_pred))

print("\n混淆矩阵:")
print(confusion_matrix(y_test, y_pred))

# 支持向量
print(f"\n支持向量数量: {len(svm_linear.support_vectors_)}")
print(f"支持向量比例: {len(svm_linear.support_vectors_) / len(X_train):.2%}")

# 特征重要性（线性 SVM）
if svm_linear.kernel == 'linear':
    feature_importance = pd.DataFrame({
        'feature': feature_names,
        'coefficient': svm_linear.coef_[0]
    }).sort_values('coefficient', key=abs, ascending=False)
    
    print("\n特征系数:")
    print(feature_importance)
```

### 8.2 RBF 核 SVM

```python
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

# 创建 RBF 核 SVM
svm_rbf = SVC(
    kernel='rbf',
    C=1.0,
    gamma='scale',  # 'scale', 'auto', 或具体数值
    random_state=42
)

# 网格搜索最优参数
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': [0.001, 0.01, 0.1, 1, 10]
}

grid_search = GridSearchCV(
    svm_rbf,
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

print(f"最优参数: {grid_search.best_params_}")
print(f"最优交叉验证得分: {grid_search.best_score_:.4f}")

# 使用最优参数预测
best_svm = grid_search.best_estimator_
y_pred = best_svm.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"测试集准确率: {accuracy:.4f}")
```

### 8.3 支持向量回归（SVR）

```python
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# 准备回归数据
# X: 特征矩阵
# y: 连续目标变量

# 标准化
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

# 创建 SVR 模型
svr = SVR(
    kernel='rbf',
    C=1.0,
    epsilon=0.1,  # ε-不敏感参数
    gamma='scale'
)

# 训练模型
svr.fit(X_train, y_train)

# 预测
y_pred = svr.predict(X_test)

# 评估
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"均方误差 (MSE): {mse:.4f}")
print(f"均方根误差 (RMSE): {rmse:.4f}")
print(f"平均绝对误差 (MAE): {mae:.4f}")
print(f"R² 分数: {r2:.4f}")

# 支持向量
print(f"\n支持向量数量: {len(svr.support_vectors_)}")
```

### 8.4 多分类 SVM

```python
from sklearn.svm import SVC
from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier

# 准备多分类数据
# y: 多类别标签 (0, 1, 2, ...)

# 标准化
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

# 方法1：一对多（OvR）
svm_ovr = OneVsRestClassifier(
    SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)
)
svm_ovr.fit(X_train, y_train)
y_pred_ovr = svm_ovr.predict(X_test)
accuracy_ovr = accuracy_score(y_test, y_pred_ovr)
print(f"OvR 准确率: {accuracy_ovr:.4f}")

# 方法2：一对一（OvO）
svm_ovo = OneVsOneClassifier(
    SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)
)
svm_ovo.fit(X_train, y_train)
y_pred_ovo = svm_ovo.predict(X_test)
accuracy_ovo = accuracy_score(y_test, y_pred_ovo)
print(f"OvO 准确率: {accuracy_ovo:.4f}")

# 方法3：直接使用 SVC（自动选择策略）
svm_multiclass = SVC(
    kernel='rbf',
    C=1.0,
    gamma='scale',
    decision_function_shape='ovo',  # 或 'ovr'
    random_state=42
)
svm_multiclass.fit(X_train, y_train)
y_pred_multiclass = svm_multiclass.predict(X_test)
accuracy_multiclass = accuracy_score(y_test, y_pred_multiclass)
print(f"多分类 SVM 准确率: {accuracy_multiclass:.4f}")
```

### 8.5 量化交易应用示例

```python
import yfinance as yf
import pandas as pd
import numpy as np
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score
import matplotlib.pyplot as plt

def get_stock_data(symbol, period='3y'):
    """获取股票数据"""
    stock = yf.Ticker(symbol)
    data = stock.history(period=period)
    return data

def calculate_features(data):
    """计算技术指标特征"""
    df = data.copy()
    
    # 收益率
    df['Return'] = df['Close'].pct_change()
    
    # RSI
    delta = df['Close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df['RSI'] = 100 - (100 / (1 + rs))
    
    # MACD
    exp1 = df['Close'].ewm(span=12, adjust=False).mean()
    exp2 = df['Close'].ewm(span=26, adjust=False).mean()
    df['MACD'] = exp1 - exp2
    df['MACD_signal'] = df['MACD'].ewm(span=9, adjust=False).mean()
    df['MACD_diff'] = df['MACD'] - df['MACD_signal']
    
    # 均线
    df['MA_5'] = df['Close'].rolling(window=5).mean()
    df['MA_20'] = df['Close'].rolling(window=20).mean()
    df['MA_50'] = df['Close'].rolling(window=50).mean()
    df['MA_diff_5_20'] = (df['MA_5'] - df['MA_20']) / df['Close']
    df['MA_diff_20_50'] = (df['MA_20'] - df['MA_50']) / df['Close']
    
    # 波动率
    df['Volatility_5'] = df['Return'].rolling(window=5).std()
    df['Volatility_20'] = df['Return'].rolling(window=20).std()
    
    # 成交量
    df['Volume_MA'] = df['Volume'].rolling(window=20).mean()
    df['Volume_Ratio'] = df['Volume'] / df['Volume_MA']
    
    # 价格位置
    df['High_Low_Ratio'] = (df['High'] - df['Low']) / df['Close']
    df['Close_MA_Ratio'] = df['Close'] / df['MA_20']
    
    return df

def create_target(data, horizon=5):
    """创建目标变量（未来是否上涨）"""
    data['Future_Return'] = data['Close'].pct_change(horizon).shift(-horizon)
    data['Target'] = (data['Future_Return'] > 0).astype(int)
    return data

def compare_svm_kernels(X_train, X_test, y_train, y_test):
    """比较不同核函数的 SVM"""
    
    # 标准化
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    results = {}
    
    # 线性核
    print("=" * 60)
    print("线性核 SVM")
    print("=" * 60)
    
    svm_linear = SVC(kernel='linear', C=1.0, random_state=42)
    svm_linear.fit(X_train_scaled, y_train)
    y_pred_linear = svm_linear.predict(X_test_scaled)
    y_proba_linear = svm_linear.decision_function(X_test_scaled)
    
    accuracy_linear = accuracy_score(y_test, y_pred_linear)
    auc_linear = roc_auc_score(y_test, y_proba_linear)
    
    print(f"准确率: {accuracy_linear:.4f}")
    print(f"AUC: {auc_linear:.4f}")
    print(f"支持向量数量: {len(svm_linear.support_vectors_)}")
    results['Linear'] = {
        'accuracy': accuracy_linear,
        'auc': auc_linear,
        'n_sv': len(svm_linear.support_vectors_)
    }
    
    # RBF 核
    print("\n" + "=" * 60)
    print("RBF 核 SVM")
    print("=" * 60)
    
    # 参数调优
    param_grid = {
        'C': [0.1, 1, 10, 100],
        'gamma': [0.001, 0.01, 0.1, 1]
    }
    
    svm_rbf = SVC(kernel='rbf', random_state=42)
    grid_search = GridSearchCV(
        svm_rbf, param_grid, cv=5, scoring='accuracy', n_jobs=-1
    )
    grid_search.fit(X_train_scaled, y_train)
    
    print(f"最优参数: {grid_search.best_params_}")
    
    best_svm_rbf = grid_search.best_estimator_
    y_pred_rbf = best_svm_rbf.predict(X_test_scaled)
    y_proba_rbf = best_svm_rbf.decision_function(X_test_scaled)
    
    accuracy_rbf = accuracy_score(y_test, y_pred_rbf)
    auc_rbf = roc_auc_score(y_test, y_proba_rbf)
    
    print(f"准确率: {accuracy_rbf:.4f}")
    print(f"AUC: {auc_rbf:.4f}")
    print(f"支持向量数量: {len(best_svm_rbf.support_vectors_)}")
    results['RBF'] = {
        'accuracy': accuracy_rbf,
        'auc': auc_rbf,
        'n_sv': len(best_svm_rbf.support_vectors_)
    }
    
    # 比较结果
    print("\n" + "=" * 60)
    print("方法比较")
    print("=" * 60)
    for kernel, metrics in results.items():
        print(f"{kernel}: 准确率={metrics['accuracy']:.4f}, "
              f"AUC={metrics['auc']:.4f}, "
              f"支持向量数={metrics['n_sv']}")
    
    return svm_linear, best_svm_rbf, scaler, results

def plot_decision_boundary(X, y, model, scaler, feature_names, idx1=0, idx2=1):
    """绘制决策边界（二维特征）"""
    # 只使用两个特征进行可视化
    X_2d = X[:, [idx1, idx2]]
    X_2d_scaled = scaler.transform(X)[:, [idx1, idx2]]
    
    # 创建网格
    h = 0.02
    x_min, x_max = X_2d_scaled[:, 0].min() - 1, X_2d_scaled[:, 0].max() + 1
    y_min, y_max = X_2d_scaled[:, 1].min() - 1, X_2d_scaled[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    # 预测网格点
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    # 绘制
    plt.figure(figsize=(10, 8))
    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)
    scatter = plt.scatter(X_2d_scaled[:, 0], X_2d_scaled[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolors='k')
    plt.xlabel(f'{feature_names[idx1]} (标准化)', fontsize=12)
    plt.ylabel(f'{feature_names[idx2]} (标准化)', fontsize=12)
    plt.title('SVM 决策边界', fontsize=14)
    plt.colorbar(scatter)
    plt.tight_layout()
    plt.show()

def main():
    # 获取数据
    data = get_stock_data('AAPL', period='3y')
    
    # 计算特征
    data = calculate_features(data)
    
    # 创建目标变量
    data = create_target(data, horizon=5)
    
    # 选择特征
    feature_cols = [
        'RSI', 'MACD', 'MACD_diff',
        'MA_diff_5_20', 'MA_diff_20_50',
        'Volatility_5', 'Volatility_20',
        'Volume_Ratio',
        'High_Low_Ratio', 'Close_MA_Ratio'
    ]
    
    # 删除缺失值
    data = data[feature_cols + ['Target']].dropna()
    
    X = data[feature_cols].values
    y = data['Target'].values
    
    print(f"样本数: {len(X)}")
    print(f"正样本比例: {y.mean():.2%}")
    
    # 时间序列划分
    split_idx = int(len(X) * 0.8)
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    # 比较不同核函数
    svm_linear, svm_rbf, scaler, results = compare_svm_kernels(
        X_train, X_test, y_train, y_test
    )
    
    return svm_linear, svm_rbf, data, results

if __name__ == '__main__':
    svm_linear_model, svm_rbf_model, data, results = main()
```

### 8.6 支持向量可视化

```python
import matplotlib.pyplot as plt

def plot_support_vectors(X, y, model, scaler, feature_names, idx1=0, idx2=1):
    """可视化支持向量"""
    X_scaled = scaler.transform(X)
    X_2d = X_scaled[:, [idx1, idx2]]
    
    # 获取支持向量
    support_vectors = model.support_vectors_[:, [idx1, idx2]]
    support_indices = model.support_
    
    plt.figure(figsize=(10, 8))
    
    # 绘制所有样本
    scatter1 = plt.scatter(
        X_2d[y == 0, 0], X_2d[y == 0, 1],
        c='blue', marker='o', alpha=0.3, label='类别 0'
    )
    scatter2 = plt.scatter(
        X_2d[y == 1, 0], X_2d[y == 1, 1],
        c='red', marker='s', alpha=0.3, label='类别 1'
    )
    
    # 高亮支持向量
    plt.scatter(
        support_vectors[:, 0], support_vectors[:, 1],
        s=200, facecolors='none', edgecolors='black',
        linewidths=2, label='支持向量'
    )
    
    plt.xlabel(f'{feature_names[idx1]} (标准化)', fontsize=12)
    plt.ylabel(f'{feature_names[idx2]} (标准化)', fontsize=12)
    plt.title('支持向量可视化', fontsize=14)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    print(f"总样本数: {len(X)}")
    print(f"支持向量数: {len(support_vectors)}")
    print(f"支持向量比例: {len(support_vectors) / len(X):.2%}")

# 使用示例
# plot_support_vectors(X_train, y_train, svm_rbf_model, scaler, feature_cols)
```

---

## 九、总结

### 9.1 核心要点

**1. SVM 的本质**
- 基于结构风险最小化原理
- 通过最大化间隔提高泛化能力
- 只使用支持向量，模型简洁

**2. 主要类型**

| 类型 | 特点 | 适用场景 |
|------|------|---------|
| **线性 SVM** | 线性决策边界 | 线性可分或近似线性可分 |
| **软间隔 SVM** | 允许误分类 | 有噪声的数据 |
| **核 SVM** | 非线性决策边界 | 非线性可分 |
| **SVR** | 回归问题 | 连续值预测 |

**3. 核函数选择**
- **线性核**：线性关系
- **RBF 核**：复杂的非线性关系（最常用）
- **多项式核**：多项式关系
- **Sigmoid 核**：类似神经网络

### 9.2 优势与局限

**优势：**
- ✅ 理论基础完备
- ✅ 泛化能力强
- ✅ 处理非线性问题
- ✅ 对高维数据有效
- ✅ 稀疏解（只使用支持向量）

**局限：**
- ❌ 计算复杂度高
- ❌ 对大规模数据不友好
- ❌ 需要调参
- ❌ 对特征缩放敏感
- ❌ 可解释性较差

### 9.3 在量化交易中的应用

**主要应用：**
1. **涨跌预测**：使用 SVM 预测股价涨跌方向
2. **交易信号生成**：根据决策函数值生成交易信号
3. **市场状态识别**：识别牛市、熊市、震荡市
4. **特征选择**：线性 SVM 可以用于特征重要性分析

**注意事项：**
- 数据预处理（标准化、处理缺失值）
- 参数调优（$C$、核函数、核参数）
- 时间序列交叉验证（避免数据泄露）
- 考虑计算效率（大规模数据使用线性 SVM）

### 9.4 学习建议

**1. 理论基础**
- 理解最大间隔原理
- 理解拉格朗日对偶
- 理解核技巧
- 理解结构风险最小化

**2. 实践应用**
- 使用 scikit-learn 实现 SVM
- 比较不同核函数的性能
- 进行参数调优
- 在量化交易中应用 SVM

**3. 进阶学习**
- 学习其他核方法（核 PCA、核岭回归等）
- 学习大规模 SVM 算法（SGD、近似方法）
- 学习其他分类方法（随机森林、神经网络等）
- 学习集成方法

---

**总结：支持向量机是一种强大的分类和回归方法，通过最大化间隔和核技巧，可以处理线性和非线性问题。在量化交易中，SVM 可以用于涨跌预测、交易信号生成和市场状态识别。理解 SVM 的原理、核函数的选择和参数调优，对于构建有效的量化模型至关重要。通过合理的数据预处理、参数调优和模型验证，可以充分发挥 SVM 的优势，提高交易策略的准确性和稳定性。**

