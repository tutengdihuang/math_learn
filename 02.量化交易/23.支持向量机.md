# 支持向量机（Support Vector Machine, SVM）：原理、算法与应用

## 目录
1. [支持向量机概述](#一-支持向量机概述)
2. [线性支持向量机](#二-线性支持向量机)
3. [软间隔支持向量机](#三-软间隔支持向量机)
4. [非线性支持向量机：核方法](#四-非线性支持向量机核方法)
5. [支持向量回归（SVR）](#五-支持向量回归svr)
6. [多分类支持向量机](#六-多分类支持向量机)
7. [在量化交易中的应用](#七-在量化交易中的应用)
8. [Python 实现示例](#八-python-实现示例)
9. [总结](#九-总结)

---

## 一、支持向量机概述

### 1.1 什么是支持向量机

**支持向量机（Support Vector Machine, SVM）**是一种基于统计学习理论的监督学习算法，主要用于分类和回归问题。

**核心思想：**
- 寻找最优的**分离超平面**，使得两类样本之间的**间隔（Margin）**最大
- 利用**支持向量**（Support Vectors）定义决策边界
- 通过**核技巧**处理非线性问题

### 1.2 SVM 的发展历史

- **1960s**：Vapnik 和 Chervonenkis 提出统计学习理论
- **1990s**：Boser、Guyon 和 Vapnik 提出支持向量机
- **1995**：Cortes 和 Vapnik 提出软间隔 SVM
- **1990s-2000s**：核方法的发展，使 SVM 能够处理非线性问题

### 1.3 SVM 的主要类型

**1. 线性支持向量机（Linear SVM）**
- 硬间隔：数据线性可分
- 软间隔：数据近似线性可分

**2. 非线性支持向量机（Nonlinear SVM）**
- 使用核技巧将数据映射到高维空间
- 在高维空间中应用线性 SVM

**3. 支持向量回归（SVR）**
- 用于回归问题
- 使用 ε-不敏感损失函数

### 1.4 SVM 的优势

**主要优势：**
- ✅ **理论基础完备**：基于统计学习理论和结构风险最小化
- ✅ **泛化能力强**：通过最大化间隔提高泛化能力
- ✅ **处理非线性问题**：通过核技巧处理复杂的非线性关系
- ✅ **稀疏解**：只使用支持向量，模型简洁
- ✅ **对高维数据有效**：在高维空间中表现良好
- ✅ **内存效率高**：只需要存储支持向量

### 1.5 SVM 的局限性

**主要局限：**
- ❌ **计算复杂度高**：训练时间复杂度为 $O(n^2)$ 或 $O(n^3)$
- ❌ **对大规模数据不友好**：样本数很大时训练慢
- ❌ **需要调参**：核函数选择和参数调优
- ❌ **对特征缩放敏感**：需要标准化特征
- ❌ **可解释性较差**：特别是使用核函数时

### 1.6 SVM 的应用领域

**主要应用：**
1. **文本分类**：垃圾邮件检测、情感分析
2. **图像识别**：手写识别、人脸识别
3. **生物信息学**：基因分类、蛋白质结构预测
4. **金融分析**：信用评分、欺诈检测
5. **量化交易**：涨跌预测、交易信号生成

---

## 二、线性支持向量机

### 2.1 问题设置

**二分类问题：**

给定训练数据集：
$$
D = \{(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \ldots, (\mathbf{x}_n, y_n)\}
$$

其中：
- $\mathbf{x}_i \in \mathbb{R}^p$：特征向量
- $y_i \in \{-1, +1\}$：类别标签

**目标：** 找到一个超平面 $f(\mathbf{x}) = \mathbf{w}' \mathbf{x} + b = 0$，将两类样本分开。

### 2.2 分离超平面

**超平面方程：**

$$
f(\mathbf{x}) = \mathbf{w}' \mathbf{x} + b = 0
$$

其中：
- $\mathbf{w} \in \mathbb{R}^p$：法向量（权重向量）
- $b \in \mathbb{R}$：偏置项

**分类规则：**

$$
\hat{y} = \begin{cases}
+1 & \text{if } f(\mathbf{x}) = \mathbf{w}' \mathbf{x} + b \geq 0 \\
-1 & \text{if } f(\mathbf{x}) = \mathbf{w}' \mathbf{x} + b < 0
\end{cases}
$$

### 2.3 函数间隔和几何间隔

**函数间隔（Functional Margin）：**

对于样本 $(\mathbf{x}_i, y_i)$，函数间隔定义为：

$$
\hat{\gamma}_i = y_i(\mathbf{w}' \mathbf{x}_i + b)
$$

**几何间隔（Geometric Margin）：**

几何间隔是样本到超平面的距离：

$$
\gamma_i = \frac{y_i(\mathbf{w}' \mathbf{x}_i + b)}{\|\mathbf{w}\|} = \frac{\hat{\gamma}_i}{\|\mathbf{w}\|}
$$

**数据集的最小间隔：**

$$
\gamma = \min_{i=1,\ldots,n} \gamma_i
$$

### 2.4 最大间隔分类器

**目标：** 找到使几何间隔最大的超平面。

**优化问题：**

$$
\begin{aligned}
\max_{\mathbf{w}, b} \quad & \gamma \\
\text{s.t.} \quad & \frac{y_i(\mathbf{w}' \mathbf{x}_i + b)}{\|\mathbf{w}\|} \geq \gamma, \quad i = 1, \ldots, n
\end{aligned}
$$

**等价形式：**

由于可以任意缩放 $\mathbf{w}$ 和 $b$，可以固定函数间隔为 1，问题转化为：

$$
\begin{aligned}
\min_{\mathbf{w}, b} \quad & \frac{1}{2}\|\mathbf{w}\|^2 \\
\text{s.t.} \quad & y_i(\mathbf{w}' \mathbf{x}_i + b) \geq 1, \quad i = 1, \ldots, n
\end{aligned}
$$

这是一个**凸二次规划问题**。

### 2.5 拉格朗日对偶

**原始问题（Primal Problem）：**

$$
\begin{aligned}
\min_{\mathbf{w}, b} \quad & \frac{1}{2}\|\mathbf{w}\|^2 \\
\text{s.t.} \quad & y_i(\mathbf{w}' \mathbf{x}_i + b) - 1 \geq 0, \quad i = 1, \ldots, n
\end{aligned}
$$

**拉格朗日函数：**

$$
L(\mathbf{w}, b, \boldsymbol{\alpha}) = \frac{1}{2}\|\mathbf{w}\|^2 - \sum_{i=1}^{n} \alpha_i [y_i(\mathbf{w}' \mathbf{x}_i + b) - 1]
$$

其中 $\alpha_i \geq 0$ 是拉格朗日乘数。

**对偶问题（Dual Problem）：**

$$
\begin{aligned}
\max_{\boldsymbol{\alpha}} \quad & \sum_{i=1}^{n} \alpha_i - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_i \alpha_j y_i y_j \mathbf{x}_i' \mathbf{x}_j \\
\text{s.t.} \quad & \sum_{i=1}^{n} \alpha_i y_i = 0 \\
& \alpha_i \geq 0, \quad i = 1, \ldots, n
\end{aligned}
$$

### 2.6 KKT 条件

**KKT（Karush-Kuhn-Tucker）条件：**

1. **原始可行性**：
   $$
   y_i(\mathbf{w}' \mathbf{x}_i + b) - 1 \geq 0, \quad i = 1, \ldots, n
   $$

2. **对偶可行性**：
   $$
   \alpha_i \geq 0, \quad i = 1, \ldots, n
   $$

3. **互补松弛性**：
   $$
   \alpha_i [y_i(\mathbf{w}' \mathbf{x}_i + b) - 1] = 0, \quad i = 1, \ldots, n
   $$

4. **梯度条件**：
   $$
   \frac{\partial L}{\partial \mathbf{w}} = \mathbf{w} - \sum_{i=1}^{n} \alpha_i y_i \mathbf{x}_i = 0
   $$
   $$
   \frac{\partial L}{\partial b} = -\sum_{i=1}^{n} \alpha_i y_i = 0
   $$

### 2.7 支持向量

**支持向量的定义：**

根据互补松弛性，如果 $\alpha_i > 0$，则：

$$
y_i(\mathbf{w}' \mathbf{x}_i + b) - 1 = 0
$$

即样本 $\mathbf{x}_i$ 位于间隔边界上，这样的样本称为**支持向量（Support Vectors）**。

**权重向量的表示：**

从梯度条件得到：

$$
\mathbf{w} = \sum_{i=1}^{n} \alpha_i y_i \mathbf{x}_i = \sum_{i \in SV} \alpha_i y_i \mathbf{x}_i
$$

其中 $SV$ 是支持向量的集合。**权重向量只由支持向量决定**。

**偏置项的求解：**

对于任意支持向量 $\mathbf{x}_s$：

$$
b = y_s - \mathbf{w}' \mathbf{x}_s = y_s - \sum_{i \in SV} \alpha_i y_i \mathbf{x}_i' \mathbf{x}_s
$$

### 2.8 决策函数

**决策函数：**

$$
f(\mathbf{x}) = \mathbf{w}' \mathbf{x} + b = \sum_{i \in SV} \alpha_i y_i \mathbf{x}_i' \mathbf{x} + b
$$

**分类规则：**

$$
\hat{y} = \text{sign}(f(\mathbf{x}))
$$

### 2.9 硬间隔 SVM 的优缺点

**优点：**
- ✅ 理论完备，有最优解
- ✅ 只使用支持向量，模型简洁
- ✅ 泛化能力强

**缺点：**
- ❌ 要求数据严格线性可分
- ❌ 对噪声和异常值敏感
- ❌ 实际应用中很少满足线性可分条件

---

## 三、软间隔支持向量机

### 3.1 软间隔的动机

**硬间隔的问题：**
- 要求数据严格线性可分
- 对噪声和异常值敏感
- 可能过拟合

**软间隔的解决方案：**
- 允许一些样本违反间隔约束
- 引入**松弛变量（Slack Variables）**
- 在最大化间隔和最小化分类错误之间平衡

### 3.2 软间隔优化问题

#### 3.2.1 软间隔最大化的原始问题

**问题描述：**

对于近似线性可分的数据，允许一些样本违反间隔约束，引入松弛变量 $\xi_i \geq 0$。

**原始优化问题：**

$$
\begin{aligned}
\min_{\mathbf{w}, b, \boldsymbol{\xi}} \quad & \frac{1}{2}\|\mathbf{w}\|^2 + C \sum_{i=1}^{n} \xi_i \\
\text{s.t.} \quad & y_i(\mathbf{w}' \mathbf{x}_i + b) \geq 1 - \xi_i, \quad i = 1, \ldots, n \\
& \xi_i \geq 0, \quad i = 1, \ldots, n
\end{aligned}
$$

其中：
- $\xi_i$：松弛变量（Slack Variable），表示样本 $\mathbf{x}_i$ 违反间隔约束的程度
- $C > 0$：惩罚参数（Penalty Parameter），控制对误分类的惩罚程度
- $\boldsymbol{\xi} = (\xi_1, \xi_2, \ldots, \xi_n)'$：松弛变量向量

#### 3.2.2 松弛变量的几何意义

**松弛变量的含义：**

- **$\xi_i = 0$**：样本 $\mathbf{x}_i$ 满足硬间隔约束，位于间隔边界上或正确一侧
- **$0 < \xi_i < 1$**：样本 $\mathbf{x}_i$ 位于间隔内，但在正确一侧
- **$\xi_i = 1$**：样本 $\mathbf{x}_i$ 位于分离超平面上
- **$\xi_i > 1$**：样本 $\mathbf{x}_i$ 被误分类，位于错误一侧

**几何解释：**

$$
\xi_i = \max(0, 1 - y_i(\mathbf{w}' \mathbf{x}_i + b))
$$

这正好是**合页损失函数**的值。

#### 3.2.3 目标函数的解释

**目标函数：**

$$
\frac{1}{2}\|\mathbf{w}\|^2 + C \sum_{i=1}^{n} \xi_i
$$

**两部分：**

1. **$\frac{1}{2}\|\mathbf{w}\|^2$**：正则化项，控制模型的复杂度
   - 最小化 $\|\mathbf{w}\|$ 等价于最大化间隔 $\frac{2}{\|\mathbf{w}\|}$
   - 防止过拟合

2. **$C \sum_{i=1}^{n} \xi_i$**：经验风险项，惩罚违反约束的样本
   - $C$ 控制正则化项和经验风险项的平衡
   - $C$ 越大，对误分类的惩罚越大

#### 3.2.4 参数 $C$ 的作用

**参数 $C$ 的几何意义：**

- **$C \to \infty$**：软间隔退化为硬间隔
  - 不允许任何样本违反约束
  - 要求数据严格线性可分
  - 可能过拟合

- **$C \to 0$**：几乎不惩罚误分类
  - 间隔可以任意大
  - 可能欠拟合
  - 所有样本都可能成为支持向量

- **$C$ 适中**：在间隔大小和分类错误之间平衡
  - 允许少量样本违反约束
  - 提高模型的鲁棒性

**参数 $C$ 的选择：**
- **$C$ 很大**：对误分类惩罚大，间隔小，可能过拟合
- **$C$ 很小**：对误分类惩罚小，间隔大，可能欠拟合
- **最优 $C$**：通过交叉验证选择

### 3.3 软间隔最大化的对偶问题

#### 3.3.1 拉格朗日函数

**引入拉格朗日乘数：**

- $\alpha_i \geq 0$：对应约束 $y_i(\mathbf{w}' \mathbf{x}_i + b) \geq 1 - \xi_i$
- $\mu_i \geq 0$：对应约束 $\xi_i \geq 0$

**拉格朗日函数：**

$$
\begin{aligned}
L(\mathbf{w}, b, \boldsymbol{\xi}, \boldsymbol{\alpha}, \boldsymbol{\mu}) &= \frac{1}{2}\|\mathbf{w}\|^2 + C \sum_{i=1}^{n} \xi_i \\
&\quad - \sum_{i=1}^{n} \alpha_i [y_i(\mathbf{w}' \mathbf{x}_i + b) - 1 + \xi_i] \\
&\quad - \sum_{i=1}^{n} \mu_i \xi_i
\end{aligned}
$$

其中：
- $\boldsymbol{\alpha} = (\alpha_1, \alpha_2, \ldots, \alpha_n)'$：拉格朗日乘数向量
- $\boldsymbol{\mu} = (\mu_1, \mu_2, \ldots, \mu_n)'$：拉格朗日乘数向量

#### 3.3.2 对偶问题的推导

**步骤1：求拉格朗日函数对原始变量的偏导数**

**对 $\mathbf{w}$ 求偏导：**

$$
\frac{\partial L}{\partial \mathbf{w}} = \mathbf{w} - \sum_{i=1}^{n} \alpha_i y_i \mathbf{x}_i = 0
$$

得到：

$$
\mathbf{w} = \sum_{i=1}^{n} \alpha_i y_i \mathbf{x}_i
$$

**对 $b$ 求偏导：**

$$
\frac{\partial L}{\partial b} = -\sum_{i=1}^{n} \alpha_i y_i = 0
$$

得到：

$$
\sum_{i=1}^{n} \alpha_i y_i = 0
$$

**对 $\xi_i$ 求偏导：**

$$
\frac{\partial L}{\partial \xi_i} = C - \alpha_i - \mu_i = 0
$$

得到：

$$
\alpha_i + \mu_i = C
$$

由于 $\mu_i \geq 0$，因此：

$$
\alpha_i \leq C
$$

**步骤2：将结果代入拉格朗日函数**

将 $\mathbf{w} = \sum_{i=1}^{n} \alpha_i y_i \mathbf{x}_i$ 代入拉格朗日函数：

$$
\begin{aligned}
L(\mathbf{w}, b, \boldsymbol{\xi}, \boldsymbol{\alpha}, \boldsymbol{\mu}) &= \frac{1}{2}\left\|\sum_{i=1}^{n} \alpha_i y_i \mathbf{x}_i\right\|^2 + C \sum_{i=1}^{n} \xi_i \\
&\quad - \sum_{i=1}^{n} \alpha_i \left[y_i\left(\sum_{j=1}^{n} \alpha_j y_j \mathbf{x}_j' \mathbf{x}_i + b\right) - 1 + \xi_i\right] \\
&\quad - \sum_{i=1}^{n} \mu_i \xi_i
\end{aligned}
$$

展开并整理：

$$
\begin{aligned}
&= \frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n} \alpha_i \alpha_j y_i y_j \mathbf{x}_i' \mathbf{x}_j + C \sum_{i=1}^{n} \xi_i \\
&\quad - \sum_{i=1}^{n}\sum_{j=1}^{n} \alpha_i \alpha_j y_i y_j \mathbf{x}_i' \mathbf{x}_j - b \sum_{i=1}^{n} \alpha_i y_i + \sum_{i=1}^{n} \alpha_i - \sum_{i=1}^{n} \alpha_i \xi_i \\
&\quad - \sum_{i=1}^{n} \mu_i \xi_i
\end{aligned}
$$

利用 $\sum_{i=1}^{n} \alpha_i y_i = 0$ 和 $\alpha_i + \mu_i = C$：

$$
\begin{aligned}
&= -\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n} \alpha_i \alpha_j y_i y_j \mathbf{x}_i' \mathbf{x}_j + \sum_{i=1}^{n} \alpha_i + \sum_{i=1}^{n} (C - \alpha_i - \mu_i) \xi_i \\
&= -\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n} \alpha_i \alpha_j y_i y_j \mathbf{x}_i' \mathbf{x}_j + \sum_{i=1}^{n} \alpha_i
\end{aligned}
$$

**步骤3：得到对偶问题**

**对偶问题：**

$$
\begin{aligned}
\max_{\boldsymbol{\alpha}} \quad & \sum_{i=1}^{n} \alpha_i - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_i \alpha_j y_i y_j \mathbf{x}_i' \mathbf{x}_j \\
\text{s.t.} \quad & \sum_{i=1}^{n} \alpha_i y_i = 0 \\
& 0 \leq \alpha_i \leq C, \quad i = 1, \ldots, n
\end{aligned}
$$

**与硬间隔的区别：**
- 硬间隔：$0 \leq \alpha_i < \infty$
- 软间隔：$0 \leq \alpha_i \leq C$（增加了上界约束）

#### 3.3.3 对偶问题的等价形式

**最小化形式：**

$$
\begin{aligned}
\min_{\boldsymbol{\alpha}} \quad & \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_i \alpha_j y_i y_j \mathbf{x}_i' \mathbf{x}_j - \sum_{i=1}^{n} \alpha_i \\
\text{s.t.} \quad & \sum_{i=1}^{n} \alpha_i y_i = 0 \\
& 0 \leq \alpha_i \leq C, \quad i = 1, \ldots, n
\end{aligned}
$$

这是一个**凸二次规划问题**，可以使用标准优化算法求解。

### 3.4 软间隔的 KKT 条件

#### 3.4.1 完整的 KKT 条件

**KKT（Karush-Kuhn-Tucker）条件：**

1. **原始可行性（Primal Feasibility）**：
   $$
   y_i(\mathbf{w}' \mathbf{x}_i + b) - 1 + \xi_i \geq 0, \quad i = 1, \ldots, n
   $$
   $$
   \xi_i \geq 0, \quad i = 1, \ldots, n
   $$

2. **对偶可行性（Dual Feasibility）**：
   $$
   \alpha_i \geq 0, \quad i = 1, \ldots, n
   $$
   $$
   \mu_i \geq 0, \quad i = 1, \ldots, n
   $$

3. **互补松弛性（Complementary Slackness）**：
   $$
   \alpha_i [y_i(\mathbf{w}' \mathbf{x}_i + b) - 1 + \xi_i] = 0, \quad i = 1, \ldots, n
   $$
   $$
   \mu_i \xi_i = 0, \quad i = 1, \ldots, n
   $$

4. **梯度条件（Stationarity）**：
   $$
   \frac{\partial L}{\partial \mathbf{w}} = \mathbf{w} - \sum_{i=1}^{n} \alpha_i y_i \mathbf{x}_i = 0
   $$
   $$
   \frac{\partial L}{\partial b} = -\sum_{i=1}^{n} \alpha_i y_i = 0
   $$
   $$
   \frac{\partial L}{\partial \xi_i} = C - \alpha_i - \mu_i = 0, \quad i = 1, \ldots, n
   $$

#### 3.4.2 KKT 条件的分析

**从互补松弛性可以得到：**

1. **如果 $\alpha_i > 0$**：
   $$
   y_i(\mathbf{w}' \mathbf{x}_i + b) - 1 + \xi_i = 0
   $$
   即样本 $\mathbf{x}_i$ 是支持向量。

2. **如果 $\mu_i > 0$**：
   $$
   \xi_i = 0
   $$
   即样本 $\mathbf{x}_i$ 没有违反约束。

3. **如果 $\xi_i > 0$**：
   $$
   \mu_i = 0
   $$
   结合 $\alpha_i + \mu_i = C$，得到 $\alpha_i = C$。

### 3.5 支持向量的分类

**根据 $\alpha_i$ 和 $\xi_i$ 的值，支持向量分为三类：**

#### 3.5.1 间隔支持向量（Margin Support Vectors）

**条件：** $0 < \alpha_i < C$

**性质：**
- 从 $\alpha_i + \mu_i = C$ 和 $\alpha_i < C$ 得到 $\mu_i > 0$
- 从 $\mu_i \xi_i = 0$ 和 $\mu_i > 0$ 得到 $\xi_i = 0$
- 从 $\alpha_i [y_i(\mathbf{w}' \mathbf{x}_i + b) - 1 + \xi_i] = 0$ 和 $\alpha_i > 0, \xi_i = 0$ 得到：
  $$
  y_i(\mathbf{w}' \mathbf{x}_i + b) = 1
  $$

**几何意义：**
- 位于间隔边界上（$f(\mathbf{x}_i) = \pm 1$）
- 没有违反约束（$\xi_i = 0$）
- 对确定分离超平面起关键作用

#### 3.5.2 误分类支持向量（Misclassified Support Vectors）

**条件：** $\alpha_i = C$

**性质：**
- 从 $\alpha_i + \mu_i = C$ 和 $\alpha_i = C$ 得到 $\mu_i = 0$
- 从 $\mu_i \xi_i = 0$ 和 $\mu_i = 0$ 得到 $\xi_i \geq 0$（可能大于 0）
- 从 $\alpha_i [y_i(\mathbf{w}' \mathbf{x}_i + b) - 1 + \xi_i] = 0$ 和 $\alpha_i = C > 0$ 得到：
  $$
  y_i(\mathbf{w}' \mathbf{x}_i + b) = 1 - \xi_i
  $$

**进一步分类：**

- **如果 $0 < \xi_i < 1$**：
  - $0 < y_i(\mathbf{w}' \mathbf{x}_i + b) < 1$
  - 位于间隔内，但在正确一侧

- **如果 $\xi_i = 1$**：
  - $y_i(\mathbf{w}' \mathbf{x}_i + b) = 0$
  - 位于分离超平面上

- **如果 $\xi_i > 1$**：
  - $y_i(\mathbf{w}' \mathbf{x}_i + b) < 0$
  - 被误分类，位于错误一侧

**几何意义：**
- 违反间隔约束（$\xi_i > 0$）
- 可能被误分类
- 对模型有重要影响（因为 $\alpha_i = C$ 达到上界）

#### 3.5.3 非支持向量（Non-Support Vectors）

**条件：** $\alpha_i = 0$

**性质：**
- 从 $\alpha_i + \mu_i = C$ 和 $\alpha_i = 0$ 得到 $\mu_i = C > 0$
- 从 $\mu_i \xi_i = 0$ 和 $\mu_i > 0$ 得到 $\xi_i = 0$
- 从原始约束得到：
  $$
  y_i(\mathbf{w}' \mathbf{x}_i + b) \geq 1
  $$

**几何意义：**
- 位于正确一侧，远离间隔边界
- 满足硬间隔约束
- 对决策函数没有贡献（因为 $\alpha_i = 0$）

#### 3.5.4 支持向量分类总结

| 类型 | $\alpha_i$ | $\xi_i$ | $\mu_i$ | 位置 | 对决策函数的影响 |
|------|-----------|---------|---------|------|----------------|
| **间隔支持向量** | $0 < \alpha_i < C$ | $= 0$ | $> 0$ | 间隔边界上 | 关键作用 |
| **误分类支持向量** | $= C$ | $\geq 0$ | $= 0$ | 间隔内或错误一侧 | 重要影响 |
| **非支持向量** | $= 0$ | $= 0$ | $= C$ | 正确一侧，远离边界 | 无影响 |

### 3.6 软间隔的决策函数和偏置

#### 3.6.1 权重向量

**从梯度条件得到：**

$$
\mathbf{w} = \sum_{i=1}^{n} \alpha_i y_i \mathbf{x}_i = \sum_{i \in SV} \alpha_i y_i \mathbf{x}_i
$$

其中 $SV = \{i : \alpha_i > 0\}$ 是支持向量的集合。

**权重向量只由支持向量决定。**

#### 3.6.2 偏置项的求解

**方法1：使用间隔支持向量**

对于任意间隔支持向量 $\mathbf{x}_s$（满足 $0 < \alpha_s < C$）：

$$
y_s(\mathbf{w}' \mathbf{x}_s + b) = 1
$$

因此：

$$
b = y_s - \mathbf{w}' \mathbf{x}_s = y_s - \sum_{i \in SV} \alpha_i y_i \mathbf{x}_i' \mathbf{x}_s
$$

**方法2：使用所有间隔支持向量的平均值**

$$
b = \frac{1}{|SV_M|} \sum_{s \in SV_M} \left(y_s - \sum_{i \in SV} \alpha_i y_i \mathbf{x}_i' \mathbf{x}_s\right)
$$

其中 $SV_M = \{i : 0 < \alpha_i < C\}$ 是间隔支持向量的集合。

#### 3.6.3 决策函数

**决策函数：**

$$
f(\mathbf{x}) = \mathbf{w}' \mathbf{x} + b = \sum_{i \in SV} \alpha_i y_i \mathbf{x}_i' \mathbf{x} + b
$$

**分类规则：**

$$
\hat{y} = \begin{cases}
+1 & \text{if } f(\mathbf{x}) \geq 0 \\
-1 & \text{if } f(\mathbf{x}) < 0
\end{cases}
$$

### 3.7 软间隔最大化的几何解释

#### 3.7.1 间隔的概念

**软间隔：**

允许一些样本位于间隔内或错误一侧，但通过惩罚项 $C \sum_{i=1}^{n} \xi_i$ 控制违反程度。

**间隔宽度：**

$$
\text{Margin} = \frac{2}{\|\mathbf{w}\|}
$$

**目标：**
- 最大化间隔（最小化 $\|\mathbf{w}\|$）
- 最小化违反约束的程度（最小化 $\sum_{i=1}^{n} \xi_i$）
- 在两者之间平衡（通过参数 $C$）

#### 3.7.2 参数 $C$ 的几何影响

**$C$ 很大时：**
- 惩罚项占主导
- 几乎不允许违反约束
- 间隔较小
- 决策边界更复杂

**$C$ 很小时：**
- 正则化项占主导
- 允许更多违反约束
- 间隔较大
- 决策边界更平滑

**最优 $C$：**
- 在验证集上表现最好
- 平衡偏差和方差

### 3.8 损失函数视角

#### 3.8.1 合页损失函数

**软间隔 SVM 等价于最小化以下损失函数：**

$$
\min_{\mathbf{w}, b} \quad \frac{1}{2}\|\mathbf{w}\|^2 + C \sum_{i=1}^{n} \max(0, 1 - y_i(\mathbf{w}' \mathbf{x}_i + b))
$$

其中 $\max(0, 1 - y_i(\mathbf{w}' \mathbf{x}_i + b))$ 是**合页损失（Hinge Loss）**。

**合页损失函数：**

$$
L_{\text{hinge}}(y, f(\mathbf{x})) = \max(0, 1 - y f(\mathbf{x}))
$$

**合页损失的性质：**

- 如果 $y f(\mathbf{x}) \geq 1$（正确分类且远离边界）：损失为 0
- 如果 $0 < y f(\mathbf{x}) < 1$（正确分类但在间隔内）：损失为 $1 - y f(\mathbf{x})$
- 如果 $y f(\mathbf{x}) \leq 0$（误分类）：损失为 $1 - y f(\mathbf{x}) > 1$

#### 3.8.2 合页损失与松弛变量的关系

**松弛变量 $\xi_i$ 的定义：**

$$
\xi_i = \max(0, 1 - y_i(\mathbf{w}' \mathbf{x}_i + b))
$$

这正是合页损失函数的值。

**等价性证明：**

原始问题中的约束：
$$
y_i(\mathbf{w}' \mathbf{x}_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0
$$

等价于：
$$
\xi_i \geq 1 - y_i(\mathbf{w}' \mathbf{x}_i + b), \quad \xi_i \geq 0
$$

由于要最小化 $\sum_{i=1}^{n} \xi_i$，最优的 $\xi_i$ 为：
$$
\xi_i = \max(0, 1 - y_i(\mathbf{w}' \mathbf{x}_i + b))
$$

因此，软间隔 SVM 等价于最小化正则化的合页损失。

#### 3.8.3 与其他损失函数的比较

**常用损失函数：**

1. **0-1 损失**：
   $$
   L_{0-1}(y, f(\mathbf{x})) = \begin{cases}
   0 & \text{if } y f(\mathbf{x}) > 0 \\
   1 & \text{if } y f(\mathbf{x}) \leq 0
   \end{cases}
   $$
   - 非凸，难以优化

2. **合页损失**：
   $$
   L_{\text{hinge}}(y, f(\mathbf{x})) = \max(0, 1 - y f(\mathbf{x}))
   $$
   - 凸函数，易于优化
   - 对间隔内的样本也给予惩罚

3. **逻辑损失（Logistic Loss）**：
   $$
   L_{\text{logistic}}(y, f(\mathbf{x})) = \ln(1 + \exp(-y f(\mathbf{x})))
   $$
   - 平滑，处处可导
   - 对所有样本都给予惩罚

4. **指数损失（Exponential Loss）**：
   $$
   L_{\text{exp}}(y, f(\mathbf{x})) = \exp(-y f(\mathbf{x}))
   $$
   - 对误分类样本惩罚很大

**合页损失的优势：**
- 凸函数，有全局最优解
- 只惩罚间隔内的样本，对远离边界的样本损失为 0
- 计算简单，适合大规模优化

### 3.9 参数 $C$ 的选择

#### 3.9.1 交叉验证方法

**K 折交叉验证：**

1. 将训练集分成 $K$ 折
2. 对每个 $C$ 值：
   - 用 $K-1$ 折训练模型
   - 在剩余 1 折上评估性能
   - 计算 $K$ 折的平均性能
3. 选择平均性能最好的 $C$ 值

**时间序列交叉验证（适用于量化交易）：**

1. 按时间顺序划分数据
2. 用历史数据训练，用未来数据验证
3. 避免数据泄露

#### 3.9.2 网格搜索

**搜索范围：**

```python
C_values = [0.001, 0.01, 0.1, 1, 10, 100, 1000]
```

**对数尺度搜索：**

```python
C_values = np.logspace(-3, 3, 7)  # [0.001, 0.01, ..., 1000]
```

#### 3.9.3 经验规则

**初始值选择：**

- 从 $C = 1$ 开始
- 如果过拟合（训练集准确率高，测试集准确率低）：减小 $C$
- 如果欠拟合（训练集和测试集准确率都低）：增大 $C$

**参数 $C$ 与支持向量的关系：**

- **$C$ 很大**：支持向量较少（主要是误分类样本）
- **$C$ 很小**：支持向量较多（包括很多间隔内的样本）
- **最优 $C$**：支持向量数量适中，泛化性能最好

#### 3.9.4 参数 $C$ 的调优策略

**粗调：**

1. 在大范围搜索：$C \in \{10^{-3}, 10^{-2}, \ldots, 10^{3}\}$
2. 确定大致范围

**细调：**

1. 在粗调确定的范围附近精细搜索
2. 例如：如果粗调发现 $C = 1$ 最好，则在 $[0.1, 10]$ 范围内细调

**自动化调优：**

使用 `GridSearchCV` 或 `RandomizedSearchCV` 自动搜索最优参数。

---

## 四、非线性支持向量机：核方法

### 4.1 核方法的动机

**线性 SVM 的局限性：**
- 只能处理线性可分的数据
- 对于非线性可分的数据，需要非线性决策边界

**核方法的解决方案：**
- 将数据映射到高维特征空间
- 在高维空间中应用线性 SVM
- 通过核函数隐式计算高维空间的内积

### 4.2 特征映射

**特征映射函数：**

$$
\phi: \mathbb{R}^p \rightarrow \mathcal{H}
$$

将原始特征空间 $\mathbb{R}^p$ 映射到高维特征空间 $\mathcal{H}$。

**高维空间中的 SVM：**

在高维空间中，决策函数为：

$$
f(\mathbf{x}) = \sum_{i \in SV} \alpha_i y_i \phi(\mathbf{x}_i)' \phi(\mathbf{x}) + b
$$

### 4.3 核函数

**核函数（Kernel Function）：**

$$
\kappa(\mathbf{x}_i, \mathbf{x}_j) = \phi(\mathbf{x}_i)' \phi(\mathbf{x}_j)
$$

核函数直接计算高维空间中的内积，**不需要显式计算映射函数**。

**决策函数（使用核函数）：**

$$
f(\mathbf{x}) = \sum_{i \in SV} \alpha_i y_i \kappa(\mathbf{x}_i, \mathbf{x}) + b
$$

**对偶问题（使用核函数）：**

$$
\begin{aligned}
\max_{\boldsymbol{\alpha}} \quad & \sum_{i=1}^{n} \alpha_i - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_i \alpha_j y_i y_j \kappa(\mathbf{x}_i, \mathbf{x}_j) \\
\text{s.t.} \quad & \sum_{i=1}^{n} \alpha_i y_i = 0 \\
& 0 \leq \alpha_i \leq C, \quad i = 1, \ldots, n
\end{aligned}
$$

### 4.4 常用核函数

**1. 线性核（Linear Kernel）：**

$$
\kappa(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{x}_i' \mathbf{x}_j
$$

等价于线性 SVM。

**2. 多项式核（Polynomial Kernel）：**

$$
\kappa(\mathbf{x}_i, \mathbf{x}_j) = (\gamma \mathbf{x}_i' \mathbf{x}_j + r)^d
$$

其中：
- $d \geq 1$：多项式次数（degree），必须是正整数
- $\gamma > 0$：缩放参数（scale parameter），通常取 $1/p$ 或 $1$（$p$ 是特征维度）
- $r \geq 0$：偏置参数（bias parameter），通常取 $0$ 或 $1$

**特殊形式：**

当 $\gamma = 1, r = 0$ 时：
$$
\kappa(\mathbf{x}_i, \mathbf{x}_j) = (\mathbf{x}_i' \mathbf{x}_j)^d
$$

当 $\gamma = 1, r = 1$ 时：
$$
\kappa(\mathbf{x}_i, \mathbf{x}_j) = (\mathbf{x}_i' \mathbf{x}_j + 1)^d
$$

这是最常用的形式，称为**齐次多项式核**（$r=0$）和**非齐次多项式核**（$r>0$）。

**3. RBF 核（Radial Basis Function Kernel，高斯核）：**

$$
\kappa(\mathbf{x}_i, \mathbf{x}_j) = \exp(-\gamma \|\mathbf{x}_i - \mathbf{x}_j\|^2)
$$

其中 $\gamma > 0$ 是带宽参数。

**4. Sigmoid 核：**

$$
\kappa(\mathbf{x}_i, \mathbf{x}_j) = \tanh(\gamma \mathbf{x}_i' \mathbf{x}_j + r)
$$

### 4.5 核函数的性质

**Mercer 条件：**

核函数必须满足 Mercer 条件（正定核），即对于任意有限样本集，核矩阵 $K_{ij} = \kappa(\mathbf{x}_i, \mathbf{x}_j)$ 是半正定的。

**常用核函数都满足 Mercer 条件。**

### 4.6 多项式核的详细分析

#### 4.6.1 多项式核的特征映射

**特征映射推导：**

对于 $d=2$ 的情况（二次多项式核），假设 $\mathbf{x} = (x_1, x_2)'$：

$$
\kappa(\mathbf{x}_i, \mathbf{x}_j) = (\gamma \mathbf{x}_i' \mathbf{x}_j + r)^2
$$

展开：

$$
= \gamma^2 (\mathbf{x}_i' \mathbf{x}_j)^2 + 2\gamma r (\mathbf{x}_i' \mathbf{x}_j) + r^2
$$

对于 $\mathbf{x}_i = (x_{i1}, x_{i2})'$ 和 $\mathbf{x}_j = (x_{j1}, x_{j2})'$：

$$
= \gamma^2 (x_{i1}x_{j1} + x_{i2}x_{j2})^2 + 2\gamma r (x_{i1}x_{j1} + x_{i2}x_{j2}) + r^2
$$

$$
= \gamma^2 (x_{i1}^2 x_{j1}^2 + 2x_{i1}x_{i2}x_{j1}x_{j2} + x_{i2}^2 x_{j2}^2) + 2\gamma r (x_{i1}x_{j1} + x_{i2}x_{j2}) + r^2
$$

这等价于特征映射：

$$
\phi(\mathbf{x}) = \left(\sqrt{2\gamma} x_1, \sqrt{2\gamma} x_2, \gamma x_1^2, \sqrt{2}\gamma x_1 x_2, \gamma x_2^2, \sqrt{2r\gamma} x_1, \sqrt{2r\gamma} x_2, r\right)'
$$

**一般情况：**

对于 $d$ 次多项式核，特征映射将 $p$ 维输入映射到 $\binom{p+d}{d}$ 维特征空间。

**特征空间维度：**

- $d=1$：$p$ 维（线性）
- $d=2$：$\frac{p(p+3)}{2}$ 维
- $d=3$：$\frac{p(p+1)(p+2)}{6}$ 维
- 一般：$\binom{p+d}{d} = \frac{(p+d)!}{p!d!}$ 维

**核技巧的优势：**

通过核函数直接计算高维空间的内积，避免了显式计算高维特征映射，计算复杂度从 $O(\binom{p+d}{d})$ 降低到 $O(p)$。

#### 4.6.2 多项式核的几何意义

**决策边界的形状：**

- **$d=1$**：线性决策边界（等价于线性核）
- **$d=2$**：二次曲线（椭圆、双曲线、抛物线）
- **$d=3$**：三次曲线
- **$d$ 越大**：决策边界越复杂，可以捕捉更复杂的非线性关系

**参数的影响：**

1. **$d$（多项式次数）**：
   - **$d$ 小**：决策边界简单，可能欠拟合
   - **$d$ 大**：决策边界复杂，可能过拟合
   - **常用值**：$d \in \{2, 3, 4, 5\}$

2. **$\gamma$（缩放参数）**：
   - 控制内积的缩放
   - **$\gamma$ 大**：强调特征之间的交互
   - **$\gamma$ 小**：减弱特征之间的交互
   - **常用值**：$\gamma = 1/p$ 或 $\gamma = 1$

3. **$r$（偏置参数）**：
   - **$r=0$**：齐次多项式，只包含 $d$ 次项
   - **$r>0$**：非齐次多项式，包含所有 $\leq d$ 次项
   - **常用值**：$r = 0$ 或 $r = 1$

#### 4.6.3 多项式核的参数选择

**参数 $d$ 的选择：**

1. **从 $d=2$ 开始**：通常 $d=2$ 或 $d=3$ 已经足够
2. **交叉验证**：在 $d \in \{2, 3, 4, 5\}$ 上搜索
3. **避免过大的 $d$**：$d > 5$ 通常会导致过拟合

**参数 $\gamma$ 的选择：**

1. **默认值**：$\gamma = 1/p$（$p$ 是特征维度）
2. **标准化后**：如果特征已标准化，可以使用 $\gamma = 1$
3. **交叉验证**：在 $\gamma \in \{0.001, 0.01, 0.1, 1, 10\}$ 上搜索

**参数 $r$ 的选择：**

1. **$r=0$**：齐次多项式，通常用于特征已经包含常数项的情况
2. **$r=1$**：非齐次多项式，更常用，包含所有低次项
3. **交叉验证**：在 $r \in \{0, 1\}$ 上搜索

**参数组合搜索：**

```python
param_grid = {
    'C': [0.1, 1, 10, 100],
    'degree': [2, 3, 4],  # d
    'gamma': [0.001, 0.01, 0.1, 1],  # γ
    'coef0': [0, 1]  # r
}
```

#### 4.6.4 多项式核的优缺点

**优点：**
- ✅ 可以捕捉特征之间的交互（特别是高次项）
- ✅ 对于某些问题，多项式核比 RBF 核更合适
- ✅ 参数有明确的几何意义
- ✅ 计算相对简单

**缺点：**
- ❌ 当 $d$ 很大时，特征空间维度爆炸式增长
- ❌ 容易过拟合（特别是 $d$ 大时）
- ❌ 对异常值敏感
- ❌ 需要仔细调参

#### 4.6.5 多项式核 vs RBF 核

**对比表：**

| 特性 | 多项式核 | RBF 核 |
|------|---------|--------|
| **决策边界** | 多项式曲线 | 平滑曲线 |
| **特征交互** | 显式捕捉 | 隐式捕捉 |
| **局部性** | 全局影响 | 局部影响 |
| **参数数量** | 3个（$d, \gamma, r$） | 1个（$\gamma$） |
| **计算复杂度** | $O(p)$ | $O(p)$ |
| **适用场景** | 特征交互重要 | 局部相似性重要 |
| **过拟合风险** | 高（$d$ 大时） | 中等 |

**选择建议：**

- **使用多项式核**：
  - 特征之间的交互关系重要
  - 数据具有多项式结构
  - 需要捕捉特征的高次项

- **使用 RBF 核**：
  - 数据具有局部相似性
  - 决策边界需要平滑
  - 不确定数据的具体结构（RBF 核更通用）

### 4.7 RBF 核的参数选择

**RBF 核的参数 $\gamma$：**

- **$\gamma$ 很大**：决策边界复杂，可能过拟合
- **$\gamma$ 很小**：决策边界平滑，可能欠拟合

**选择方法：**
- 交叉验证
- 网格搜索：$\gamma \in \{10^{-3}, 10^{-2}, \ldots, 10^{3}\}$

**参数组合搜索：**

同时搜索 $C$ 和 $\gamma$：

```python
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': [0.001, 0.01, 0.1, 1, 10]
}
```

### 4.7 核技巧的优势

**优势：**
- ✅ 可以处理非线性问题
- ✅ 不需要显式计算高维映射
- ✅ 计算复杂度与特征维度无关（只与样本数有关）

**缺点：**
- ❌ 需要选择合适的核函数和参数
- ❌ 可解释性较差
- ❌ 对大规模数据计算量大

---

## 五、支持向量回归（SVR）

### 5.1 支持向量回归概述

**支持向量回归（Support Vector Regression, SVR）**是 SVM 在回归问题上的扩展。

**核心思想：**
- 寻找一个函数 $f(\mathbf{x}) = \mathbf{w}' \mathbf{x} + b$，使得大部分样本落在 $\varepsilon$-带内
- 使用 $\varepsilon$-不敏感损失函数
- 只惩罚落在 $\varepsilon$-带外的样本

### 5.2 ε-不敏感损失函数

**ε-不敏感损失：**

$$
L_\varepsilon(y, f(\mathbf{x})) = \begin{cases}
0 & \text{if } |y - f(\mathbf{x})| \leq \varepsilon \\
|y - f(\mathbf{x})| - \varepsilon & \text{otherwise}
\end{cases}
$$

**特点：**
- 如果预测误差在 $\varepsilon$ 以内，损失为 0
- 只惩罚误差超过 $\varepsilon$ 的样本

### 5.3 SVR 优化问题

**原始问题：**

$$
\begin{aligned}
\min_{\mathbf{w}, b, \boldsymbol{\xi}, \boldsymbol{\xi}^*} \quad & \frac{1}{2}\|\mathbf{w}\|^2 + C \sum_{i=1}^{n} (\xi_i + \xi_i^*) \\
\text{s.t.} \quad & y_i - (\mathbf{w}' \mathbf{x}_i + b) \leq \varepsilon + \xi_i, \quad i = 1, \ldots, n \\
& (\mathbf{w}' \mathbf{x}_i + b) - y_i \leq \varepsilon + \xi_i^*, \quad i = 1, \ldots, n \\
& \xi_i, \xi_i^* \geq 0, \quad i = 1, \ldots, n
\end{aligned}
$$

其中：
- $\xi_i$：上界违反量
- $\xi_i^*$：下界违反量
- $C$：惩罚参数
- $\varepsilon$：不敏感参数

### 5.4 SVR 对偶问题

**对偶问题：**

$$
\begin{aligned}
\max_{\boldsymbol{\alpha}, \boldsymbol{\alpha}^*} \quad & -\frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} (\alpha_i - \alpha_i^*)(\alpha_j - \alpha_j^*) \kappa(\mathbf{x}_i, \mathbf{x}_j) \\
& \quad - \varepsilon \sum_{i=1}^{n} (\alpha_i + \alpha_i^*) + \sum_{i=1}^{n} y_i (\alpha_i - \alpha_i^*) \\
\text{s.t.} \quad & \sum_{i=1}^{n} (\alpha_i - \alpha_i^*) = 0 \\
& 0 \leq \alpha_i, \alpha_i^* \leq C, \quad i = 1, \ldots, n
\end{aligned}
$$

### 5.5 SVR 决策函数

**决策函数：**

$$
f(\mathbf{x}) = \sum_{i=1}^{n} (\alpha_i - \alpha_i^*) \kappa(\mathbf{x}_i, \mathbf{x}) + b
$$

**支持向量：**

满足 $|\alpha_i - \alpha_i^*| > 0$ 的样本是支持向量。

---

## 六、多分类支持向量机

### 6.1 多分类问题

**问题设置：**

$K$ 个类别：$y_i \in \{1, 2, \ldots, K\}$

**SVM 本质上是二分类方法，需要扩展来处理多分类问题。**

### 6.2 一对多（One-vs-Rest, OvR）

**方法：**

训练 $K$ 个二分类 SVM：
- 第 $k$ 个 SVM：类别 $k$ vs 其他所有类别

**分类规则：**

$$
\hat{y} = \arg\max_{k=1,\ldots,K} f_k(\mathbf{x})
$$

其中 $f_k(\mathbf{x})$ 是第 $k$ 个 SVM 的决策函数值。

**优点：**
- 简单直观
- 只需要训练 $K$ 个分类器

**缺点：**
- 类别不平衡（一个类别 vs 多个类别）
- 可能产生多个分类器都预测为正的情况

### 6.3 一对一（One-vs-One, OvO）

**方法：**

训练 $\frac{K(K-1)}{2}$ 个二分类 SVM：
- 每对类别训练一个 SVM

**分类规则：**

使用投票机制：每个 SVM 投票，得票最多的类别获胜。

**优点：**
- 每个分类器只处理两个类别，更平衡
- 通常比 OvR 更准确

**缺点：**
- 需要训练更多分类器
- 预测时需要所有分类器投票

### 6.4 有向无环图（DAG-SVM）

**方法：**

使用有向无环图组织一对一分类器，减少预测时的计算量。

---

## 七、在量化交易中的应用

### 7.1 涨跌预测

**使用 SVM 预测股价涨跌：**

**特征：**
- 技术指标：RSI、MACD、均线等
- 价格特征：收益率、波动率等
- 市场特征：成交量、换手率等

**目标：**
- 预测未来 N 天的涨跌方向（上涨/下跌）

**方法选择：**
- **线性 SVM**：如果特征与涨跌的关系近似线性
- **RBF 核 SVM**：如果关系复杂非线性

**优势：**
- 泛化能力强
- 对高维特征有效
- 可以处理非线性关系

### 7.2 交易信号生成

**使用 SVM 生成交易信号：**

**策略：**
- 如果 $f(\mathbf{x}) > \theta$：买入信号
- 如果 $f(\mathbf{x}) < -\theta$：卖出信号
- 否则：持有

其中 $\theta$ 是阈值，可以根据风险偏好调整。

### 7.3 市场状态识别

**使用 SVM 识别市场状态：**

**市场状态：**
- 牛市：持续上涨趋势
- 熊市：持续下跌趋势
- 震荡市：横盘整理

**方法：**
- 使用多分类 SVM（OvO 或 OvR）
- 或使用多个二分类 SVM

### 7.4 特征选择

**SVM 的特征重要性：**

- 对于线性 SVM，可以通过权重向量 $\mathbf{w}$ 的绝对值判断特征重要性
- 对于非线性 SVM，特征重要性分析较困难

### 7.5 实际应用注意事项

**1. 数据预处理**
- **标准化**：SVM 对特征尺度敏感，必须标准化
- **处理缺失值**：删除或插补
- **处理异常值**：识别和处理异常值

**2. 参数调优**
- **$C$ 参数**：控制对误分类的惩罚
- **核函数选择**：线性、RBF、多项式等
- **核参数**：如 RBF 核的 $\gamma$
- 使用交叉验证选择最优参数

**3. 模型验证**
- 使用时间序列交叉验证（避免数据泄露）
- 监控模型性能
- 定期重新训练

**4. 计算效率**
- 对于大规模数据，考虑使用线性 SVM 或近似方法
- 使用增量学习或在线学习

---

## 八、Python 实现示例

### 8.1 线性 SVM

```python
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import numpy as np
import pandas as pd

# 准备数据
# X: 特征矩阵 (n_samples, n_features)
# y: 标签 (n_samples,)

# 标准化（SVM 对尺度敏感）
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

# 创建线性 SVM
svm_linear = SVC(
    kernel='linear',
    C=1.0,  # 惩罚参数
    random_state=42
)

# 训练模型
svm_linear.fit(X_train, y_train)

# 预测
y_pred = svm_linear.predict(X_test)
y_decision = svm_linear.decision_function(X_test)  # 决策函数值

# 评估
accuracy = accuracy_score(y_test, y_pred)
print(f"准确率: {accuracy:.4f}")

print("\n分类报告:")
print(classification_report(y_test, y_pred))

print("\n混淆矩阵:")
print(confusion_matrix(y_test, y_pred))

# 支持向量
print(f"\n支持向量数量: {len(svm_linear.support_vectors_)}")
print(f"支持向量比例: {len(svm_linear.support_vectors_) / len(X_train):.2%}")

# 特征重要性（线性 SVM）
if svm_linear.kernel == 'linear':
    feature_importance = pd.DataFrame({
        'feature': feature_names,
        'coefficient': svm_linear.coef_[0]
    }).sort_values('coefficient', key=abs, ascending=False)
    
    print("\n特征系数:")
    print(feature_importance)
```

### 8.2 多项式核 SVM

```python
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report

# 创建多项式核 SVM
svm_poly = SVC(
    kernel='poly',
    degree=3,  # 多项式次数 d
    gamma='scale',  # 缩放参数 γ，'scale' 表示 1/(n_features * X.var())
    coef0=1.0,  # 偏置参数 r
    C=1.0,
    random_state=42
)

# 网格搜索最优参数
param_grid = {
    'C': [0.1, 1, 10, 100],
    'degree': [2, 3, 4],  # 多项式次数
    'gamma': [0.001, 0.01, 0.1, 1],  # 缩放参数
    'coef0': [0, 1]  # 偏置参数（0=齐次，1=非齐次）
}

grid_search = GridSearchCV(
    svm_poly,
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train, y_train)

print(f"最优参数: {grid_search.best_params_}")
print(f"最优交叉验证得分: {grid_search.best_score_:.4f}")

# 使用最优参数预测
best_svm_poly = grid_search.best_estimator_
y_pred = best_svm_poly.predict(X_test)
y_proba = best_svm_poly.decision_function(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"测试集准确率: {accuracy:.4f}")

print("\n分类报告:")
print(classification_report(y_test, y_pred))

# 支持向量
print(f"\n支持向量数量: {len(best_svm_poly.support_vectors_)}")
print(f"支持向量比例: {len(best_svm_poly.support_vectors_) / len(X_train):.2%}")

# 不同次数的比较
print("\n" + "=" * 60)
print("不同多项式次数的比较")
print("=" * 60)

degrees = [2, 3, 4, 5]
results_degree = {}

for d in degrees:
    svm_temp = SVC(
        kernel='poly',
        degree=d,
        gamma='scale',
        coef0=1.0,
        C=1.0,
        random_state=42
    )
    svm_temp.fit(X_train, y_train)
    y_pred_temp = svm_temp.predict(X_test)
    accuracy_temp = accuracy_score(y_test, y_pred_temp)
    results_degree[d] = {
        'accuracy': accuracy_temp,
        'n_sv': len(svm_temp.support_vectors_)
    }
    print(f"次数 d={d}: 准确率={accuracy_temp:.4f}, "
          f"支持向量数={len(svm_temp.support_vectors_)}")

# 齐次 vs 非齐次多项式核
print("\n" + "=" * 60)
print("齐次 vs 非齐次多项式核")
print("=" * 60)

# 齐次多项式核 (r=0)
svm_homogeneous = SVC(
    kernel='poly',
    degree=3,
    gamma='scale',
    coef0=0.0,  # 齐次
    C=1.0,
    random_state=42
)
svm_homogeneous.fit(X_train, y_train)
y_pred_hom = svm_homogeneous.predict(X_test)
accuracy_hom = accuracy_score(y_test, y_pred_hom)
print(f"齐次多项式核 (r=0): 准确率={accuracy_hom:.4f}")

# 非齐次多项式核 (r=1)
svm_nonhomogeneous = SVC(
    kernel='poly',
    degree=3,
    gamma='scale',
    coef0=1.0,  # 非齐次
    C=1.0,
    random_state=42
)
svm_nonhomogeneous.fit(X_train, y_train)
y_pred_nonhom = svm_nonhomogeneous.predict(X_test)
accuracy_nonhom = accuracy_score(y_test, y_pred_nonhom)
print(f"非齐次多项式核 (r=1): 准确率={accuracy_nonhom:.4f}")
```

### 8.3 RBF 核 SVM

```python
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

# 创建 RBF 核 SVM
svm_rbf = SVC(
    kernel='rbf',
    C=1.0,
    gamma='scale',  # 'scale', 'auto', 或具体数值
    random_state=42
)

# 网格搜索最优参数
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': [0.001, 0.01, 0.1, 1, 10]
}

grid_search = GridSearchCV(
    svm_rbf,
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

print(f"最优参数: {grid_search.best_params_}")
print(f"最优交叉验证得分: {grid_search.best_score_:.4f}")

# 使用最优参数预测
best_svm = grid_search.best_estimator_
y_pred = best_svm.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"测试集准确率: {accuracy:.4f}")
```

### 8.4 多项式核 vs RBF 核对比

```python
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
import numpy as np

def compare_kernels(X_train, y_train, X_test, y_test):
    """比较多项式核和RBF核的性能"""
    
    results = {}
    
    # 多项式核（最优参数）
    print("=" * 60)
    print("多项式核 SVM")
    print("=" * 60)
    
    svm_poly = SVC(
        kernel='poly',
        degree=3,
        gamma='scale',
        coef0=1.0,
        C=1.0,
        random_state=42
    )
    
    # 交叉验证
    cv_scores_poly = cross_val_score(svm_poly, X_train, y_train, cv=5)
    print(f"交叉验证准确率: {cv_scores_poly.mean():.4f} (+/- {cv_scores_poly.std() * 2:.4f})")
    
    # 训练和测试
    svm_poly.fit(X_train, y_train)
    y_pred_poly = svm_poly.predict(X_test)
    accuracy_poly = accuracy_score(y_test, y_pred_poly)
    
    results['Polynomial'] = {
        'cv_mean': cv_scores_poly.mean(),
        'cv_std': cv_scores_poly.std(),
        'test_accuracy': accuracy_poly,
        'n_sv': len(svm_poly.support_vectors_)
    }
    
    print(f"测试集准确率: {accuracy_poly:.4f}")
    print(f"支持向量数量: {len(svm_poly.support_vectors_)}")
    
    # RBF 核（最优参数）
    print("\n" + "=" * 60)
    print("RBF 核 SVM")
    print("=" * 60)
    
    svm_rbf = SVC(
        kernel='rbf',
        gamma='scale',
        C=1.0,
        random_state=42
    )
    
    # 交叉验证
    cv_scores_rbf = cross_val_score(svm_rbf, X_train, y_train, cv=5)
    print(f"交叉验证准确率: {cv_scores_rbf.mean():.4f} (+/- {cv_scores_rbf.std() * 2:.4f})")
    
    # 训练和测试
    svm_rbf.fit(X_train, y_train)
    y_pred_rbf = svm_rbf.predict(X_test)
    accuracy_rbf = accuracy_score(y_test, y_pred_rbf)
    
    results['RBF'] = {
        'cv_mean': cv_scores_rbf.mean(),
        'cv_std': cv_scores_rbf.std(),
        'test_accuracy': accuracy_rbf,
        'n_sv': len(svm_rbf.support_vectors_)
    }
    
    print(f"测试集准确率: {accuracy_rbf:.4f}")
    print(f"支持向量数量: {len(svm_rbf.support_vectors_)}")
    
    # 对比结果
    print("\n" + "=" * 60)
    print("对比结果")
    print("=" * 60)
    print(f"{'指标':<20} {'多项式核':<15} {'RBF核':<15}")
    print("-" * 50)
    print(f"{'交叉验证准确率':<20} {results['Polynomial']['cv_mean']:.4f}        {results['RBF']['cv_mean']:.4f}")
    print(f"{'测试集准确率':<20} {results['Polynomial']['test_accuracy']:.4f}        {results['RBF']['test_accuracy']:.4f}")
    print(f"{'支持向量数量':<20} {results['Polynomial']['n_sv']:<15} {results['RBF']['n_sv']:<15}")
    
    return results, svm_poly, svm_rbf

# 使用示例
# results, svm_poly_model, svm_rbf_model = compare_kernels(X_train, y_train, X_test, y_test)
```

### 8.5 支持向量回归（SVR）

```python
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# 准备回归数据
# X: 特征矩阵
# y: 连续目标变量

# 标准化
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

# 创建 SVR 模型
svr = SVR(
    kernel='rbf',
    C=1.0,
    epsilon=0.1,  # ε-不敏感参数
    gamma='scale'
)

# 训练模型
svr.fit(X_train, y_train)

# 预测
y_pred = svr.predict(X_test)

# 评估
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"均方误差 (MSE): {mse:.4f}")
print(f"均方根误差 (RMSE): {rmse:.4f}")
print(f"平均绝对误差 (MAE): {mae:.4f}")
print(f"R² 分数: {r2:.4f}")

# 支持向量
print(f"\n支持向量数量: {len(svr.support_vectors_)}")
```

### 8.4 多分类 SVM

```python
from sklearn.svm import SVC
from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier

# 准备多分类数据
# y: 多类别标签 (0, 1, 2, ...)

# 标准化
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

# 方法1：一对多（OvR）
svm_ovr = OneVsRestClassifier(
    SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)
)
svm_ovr.fit(X_train, y_train)
y_pred_ovr = svm_ovr.predict(X_test)
accuracy_ovr = accuracy_score(y_test, y_pred_ovr)
print(f"OvR 准确率: {accuracy_ovr:.4f}")

# 方法2：一对一（OvO）
svm_ovo = OneVsOneClassifier(
    SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)
)
svm_ovo.fit(X_train, y_train)
y_pred_ovo = svm_ovo.predict(X_test)
accuracy_ovo = accuracy_score(y_test, y_pred_ovo)
print(f"OvO 准确率: {accuracy_ovo:.4f}")

# 方法3：直接使用 SVC（自动选择策略）
svm_multiclass = SVC(
    kernel='rbf',
    C=1.0,
    gamma='scale',
    decision_function_shape='ovo',  # 或 'ovr'
    random_state=42
)
svm_multiclass.fit(X_train, y_train)
y_pred_multiclass = svm_multiclass.predict(X_test)
accuracy_multiclass = accuracy_score(y_test, y_pred_multiclass)
print(f"多分类 SVM 准确率: {accuracy_multiclass:.4f}")
```

### 8.5 量化交易应用示例

```python
import yfinance as yf
import pandas as pd
import numpy as np
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score
import matplotlib.pyplot as plt

def get_stock_data(symbol, period='3y'):
    """获取股票数据"""
    stock = yf.Ticker(symbol)
    data = stock.history(period=period)
    return data

def calculate_features(data):
    """计算技术指标特征"""
    df = data.copy()
    
    # 收益率
    df['Return'] = df['Close'].pct_change()
    
    # RSI
    delta = df['Close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df['RSI'] = 100 - (100 / (1 + rs))
    
    # MACD
    exp1 = df['Close'].ewm(span=12, adjust=False).mean()
    exp2 = df['Close'].ewm(span=26, adjust=False).mean()
    df['MACD'] = exp1 - exp2
    df['MACD_signal'] = df['MACD'].ewm(span=9, adjust=False).mean()
    df['MACD_diff'] = df['MACD'] - df['MACD_signal']
    
    # 均线
    df['MA_5'] = df['Close'].rolling(window=5).mean()
    df['MA_20'] = df['Close'].rolling(window=20).mean()
    df['MA_50'] = df['Close'].rolling(window=50).mean()
    df['MA_diff_5_20'] = (df['MA_5'] - df['MA_20']) / df['Close']
    df['MA_diff_20_50'] = (df['MA_20'] - df['MA_50']) / df['Close']
    
    # 波动率
    df['Volatility_5'] = df['Return'].rolling(window=5).std()
    df['Volatility_20'] = df['Return'].rolling(window=20).std()
    
    # 成交量
    df['Volume_MA'] = df['Volume'].rolling(window=20).mean()
    df['Volume_Ratio'] = df['Volume'] / df['Volume_MA']
    
    # 价格位置
    df['High_Low_Ratio'] = (df['High'] - df['Low']) / df['Close']
    df['Close_MA_Ratio'] = df['Close'] / df['MA_20']
    
    return df

def create_target(data, horizon=5):
    """创建目标变量（未来是否上涨）"""
    data['Future_Return'] = data['Close'].pct_change(horizon).shift(-horizon)
    data['Target'] = (data['Future_Return'] > 0).astype(int)
    return data

def compare_svm_kernels(X_train, X_test, y_train, y_test):
    """比较不同核函数的 SVM"""
    
    # 标准化
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    results = {}
    
    # 线性核
    print("=" * 60)
    print("线性核 SVM")
    print("=" * 60)
    
    svm_linear = SVC(kernel='linear', C=1.0, random_state=42)
    svm_linear.fit(X_train_scaled, y_train)
    y_pred_linear = svm_linear.predict(X_test_scaled)
    y_proba_linear = svm_linear.decision_function(X_test_scaled)
    
    accuracy_linear = accuracy_score(y_test, y_pred_linear)
    auc_linear = roc_auc_score(y_test, y_proba_linear)
    
    print(f"准确率: {accuracy_linear:.4f}")
    print(f"AUC: {auc_linear:.4f}")
    print(f"支持向量数量: {len(svm_linear.support_vectors_)}")
    results['Linear'] = {
        'accuracy': accuracy_linear,
        'auc': auc_linear,
        'n_sv': len(svm_linear.support_vectors_)
    }
    
    # 多项式核
    print("\n" + "=" * 60)
    print("多项式核 SVM")
    print("=" * 60)
    
    # 参数调优
    param_grid_poly = {
        'C': [0.1, 1, 10, 100],
        'degree': [2, 3, 4],
        'gamma': [0.001, 0.01, 0.1, 1],
        'coef0': [0, 1]
    }
    
    svm_poly = SVC(kernel='poly', random_state=42)
    grid_search_poly = GridSearchCV(
        svm_poly, param_grid_poly, cv=5, scoring='accuracy', n_jobs=-1
    )
    grid_search_poly.fit(X_train_scaled, y_train)
    
    print(f"最优参数: {grid_search_poly.best_params_}")
    
    best_svm_poly = grid_search_poly.best_estimator_
    y_pred_poly = best_svm_poly.predict(X_test_scaled)
    y_proba_poly = best_svm_poly.decision_function(X_test_scaled)
    
    accuracy_poly = accuracy_score(y_test, y_pred_poly)
    auc_poly = roc_auc_score(y_test, y_proba_poly)
    
    print(f"准确率: {accuracy_poly:.4f}")
    print(f"AUC: {auc_poly:.4f}")
    print(f"支持向量数量: {len(best_svm_poly.support_vectors_)}")
    results['Polynomial'] = {
        'accuracy': accuracy_poly,
        'auc': auc_poly,
        'n_sv': len(best_svm_poly.support_vectors_)
    }
    
    # RBF 核
    print("\n" + "=" * 60)
    print("RBF 核 SVM")
    print("=" * 60)
    
    # 参数调优
    param_grid_rbf = {
        'C': [0.1, 1, 10, 100],
        'gamma': [0.001, 0.01, 0.1, 1]
    }
    
    svm_rbf = SVC(kernel='rbf', random_state=42)
    grid_search_rbf = GridSearchCV(
        svm_rbf, param_grid_rbf, cv=5, scoring='accuracy', n_jobs=-1
    )
    grid_search_rbf.fit(X_train_scaled, y_train)
    
    print(f"最优参数: {grid_search_rbf.best_params_}")
    
    best_svm_rbf = grid_search_rbf.best_estimator_
    y_pred_rbf = best_svm_rbf.predict(X_test_scaled)
    y_proba_rbf = best_svm_rbf.decision_function(X_test_scaled)
    
    accuracy_rbf = accuracy_score(y_test, y_pred_rbf)
    auc_rbf = roc_auc_score(y_test, y_proba_rbf)
    
    print(f"准确率: {accuracy_rbf:.4f}")
    print(f"AUC: {auc_rbf:.4f}")
    print(f"支持向量数量: {len(best_svm_rbf.support_vectors_)}")
    results['RBF'] = {
        'accuracy': accuracy_rbf,
        'auc': auc_rbf,
        'n_sv': len(best_svm_rbf.support_vectors_)
    }
    
    # 比较结果
    print("\n" + "=" * 60)
    print("方法比较")
    print("=" * 60)
    print(f"{'核函数':<15} {'准确率':<10} {'AUC':<10} {'支持向量数':<15}")
    print("-" * 50)
    for kernel, metrics in results.items():
        print(f"{kernel:<15} {metrics['accuracy']:.4f}     {metrics['auc']:.4f}     {metrics['n_sv']:<15}")
    
    return svm_linear, best_svm_poly, best_svm_rbf, scaler, results

def plot_decision_boundary(X, y, model, scaler, feature_names, idx1=0, idx2=1):
    """绘制决策边界（二维特征）"""
    # 只使用两个特征进行可视化
    X_2d = X[:, [idx1, idx2]]
    X_2d_scaled = scaler.transform(X)[:, [idx1, idx2]]
    
    # 创建网格
    h = 0.02
    x_min, x_max = X_2d_scaled[:, 0].min() - 1, X_2d_scaled[:, 0].max() + 1
    y_min, y_max = X_2d_scaled[:, 1].min() - 1, X_2d_scaled[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    # 预测网格点
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    # 绘制
    plt.figure(figsize=(10, 8))
    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)
    scatter = plt.scatter(X_2d_scaled[:, 0], X_2d_scaled[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolors='k')
    plt.xlabel(f'{feature_names[idx1]} (标准化)', fontsize=12)
    plt.ylabel(f'{feature_names[idx2]} (标准化)', fontsize=12)
    plt.title('SVM 决策边界', fontsize=14)
    plt.colorbar(scatter)
    plt.tight_layout()
    plt.show()

def main():
    # 获取数据
    data = get_stock_data('AAPL', period='3y')
    
    # 计算特征
    data = calculate_features(data)
    
    # 创建目标变量
    data = create_target(data, horizon=5)
    
    # 选择特征
    feature_cols = [
        'RSI', 'MACD', 'MACD_diff',
        'MA_diff_5_20', 'MA_diff_20_50',
        'Volatility_5', 'Volatility_20',
        'Volume_Ratio',
        'High_Low_Ratio', 'Close_MA_Ratio'
    ]
    
    # 删除缺失值
    data = data[feature_cols + ['Target']].dropna()
    
    X = data[feature_cols].values
    y = data['Target'].values
    
    print(f"样本数: {len(X)}")
    print(f"正样本比例: {y.mean():.2%}")
    
    # 时间序列划分
    split_idx = int(len(X) * 0.8)
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    # 比较不同核函数
    svm_linear, svm_poly, svm_rbf, scaler, results = compare_svm_kernels(
        X_train, X_test, y_train, y_test
    )
    
    # 分析多项式核的参数影响
    print("\n" + "=" * 60)
    print("多项式核参数分析")
    print("=" * 60)
    
    # 不同次数的比较
    degrees = [2, 3, 4]
    for d in degrees:
        svm_temp = SVC(
            kernel='poly',
            degree=d,
            gamma='scale',
            coef0=1.0,
            C=1.0,
            random_state=42
        )
        svm_temp.fit(X_train_scaled, y_train)
        y_pred_temp = svm_temp.predict(X_test_scaled)
        accuracy_temp = accuracy_score(y_test, y_pred_temp)
        print(f"次数 d={d}: 准确率={accuracy_temp:.4f}, "
              f"支持向量数={len(svm_temp.support_vectors_)}")
    
    return svm_linear, svm_poly, svm_rbf, data, results

if __name__ == '__main__':
    svm_linear_model, svm_poly_model, svm_rbf_model, data, results = main()
```

### 8.6 支持向量可视化

```python
import matplotlib.pyplot as plt

def plot_support_vectors(X, y, model, scaler, feature_names, idx1=0, idx2=1):
    """可视化支持向量"""
    X_scaled = scaler.transform(X)
    X_2d = X_scaled[:, [idx1, idx2]]
    
    # 获取支持向量
    support_vectors = model.support_vectors_[:, [idx1, idx2]]
    support_indices = model.support_
    
    plt.figure(figsize=(10, 8))
    
    # 绘制所有样本
    scatter1 = plt.scatter(
        X_2d[y == 0, 0], X_2d[y == 0, 1],
        c='blue', marker='o', alpha=0.3, label='类别 0'
    )
    scatter2 = plt.scatter(
        X_2d[y == 1, 0], X_2d[y == 1, 1],
        c='red', marker='s', alpha=0.3, label='类别 1'
    )
    
    # 高亮支持向量
    plt.scatter(
        support_vectors[:, 0], support_vectors[:, 1],
        s=200, facecolors='none', edgecolors='black',
        linewidths=2, label='支持向量'
    )
    
    plt.xlabel(f'{feature_names[idx1]} (标准化)', fontsize=12)
    plt.ylabel(f'{feature_names[idx2]} (标准化)', fontsize=12)
    plt.title('支持向量可视化', fontsize=14)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    print(f"总样本数: {len(X)}")
    print(f"支持向量数: {len(support_vectors)}")
    print(f"支持向量比例: {len(support_vectors) / len(X):.2%}")

# 使用示例
# plot_support_vectors(X_train, y_train, svm_rbf_model, scaler, feature_cols)
```

---

## 九、总结

### 9.1 核心要点

**1. SVM 的本质**
- 基于结构风险最小化原理
- 通过最大化间隔提高泛化能力
- 只使用支持向量，模型简洁

**2. 主要类型**

| 类型 | 特点 | 适用场景 |
|------|------|---------|
| **线性 SVM** | 线性决策边界 | 线性可分或近似线性可分 |
| **软间隔 SVM** | 允许误分类 | 有噪声的数据 |
| **核 SVM** | 非线性决策边界 | 非线性可分 |
| **SVR** | 回归问题 | 连续值预测 |

**3. 核函数选择**
- **线性核**：线性关系，计算快速
- **多项式核**：多项式关系，可以捕捉特征交互
  - 参数：$d$（次数）、$\gamma$（缩放）、$r$（偏置）
  - 适用：特征交互重要的场景
- **RBF 核**：复杂的非线性关系（最常用）
  - 参数：$\gamma$（带宽）
  - 适用：局部相似性重要的场景
- **Sigmoid 核**：类似神经网络

### 9.2 优势与局限

**优势：**
- ✅ 理论基础完备
- ✅ 泛化能力强
- ✅ 处理非线性问题
- ✅ 对高维数据有效
- ✅ 稀疏解（只使用支持向量）

**局限：**
- ❌ 计算复杂度高
- ❌ 对大规模数据不友好
- ❌ 需要调参
- ❌ 对特征缩放敏感
- ❌ 可解释性较差

### 9.3 在量化交易中的应用

**主要应用：**
1. **涨跌预测**：使用 SVM 预测股价涨跌方向
2. **交易信号生成**：根据决策函数值生成交易信号
3. **市场状态识别**：识别牛市、熊市、震荡市
4. **特征选择**：线性 SVM 可以用于特征重要性分析

**注意事项：**
- 数据预处理（标准化、处理缺失值）
- 参数调优（$C$、核函数、核参数）
- 时间序列交叉验证（避免数据泄露）
- 考虑计算效率（大规模数据使用线性 SVM）

### 9.4 学习建议

**1. 理论基础**
- 理解最大间隔原理
- 理解拉格朗日对偶
- 理解核技巧
- 理解结构风险最小化

**2. 实践应用**
- 使用 scikit-learn 实现 SVM
- 比较不同核函数的性能
- 进行参数调优
- 在量化交易中应用 SVM

**3. 进阶学习**
- 学习其他核方法（核 PCA、核岭回归等）
- 学习大规模 SVM 算法（SGD、近似方法）
- 学习其他分类方法（随机森林、神经网络等）
- 学习集成方法

### 10. 偏差-方差权衡（Bias-Variance Trade-off）

#### 10.1 定义与误差分解

**偏差（Bias）**：模型预测的期望与真实函数之间的系统性偏离，反映模型的“拟合能力”是否不足（欠拟合）。

**方差（Variance）**：模型预测对训练数据扰动（例如样本替换、噪声）的敏感程度，反映模型是否过度依赖训练集（过拟合）。

**不可约噪声（Irreducible Noise）**：来自数据生成过程本身的噪声，任何模型都无法消除。

在回归问题中，通常使用平方误差的经典误差分解：
$$
\mathbb{E}_{\mathcal{D},\mathbf{x}}\left[\big(y-\hat{f}(\mathbf{x})\big)^2\right] = \underbrace{\big(\mathbb{E}[\hat{f}(\mathbf{x})]-f(\mathbf{x})\big)^2}_{\text{Bias}^2} + \underbrace{\mathbb{V}[\hat{f}(\mathbf{x})]}_{\text{Variance}} + \underbrace{\sigma^2}_{\text{Noise}}
$$
其中：
- \(\hat{f}(\mathbf{x})\) 为学习到的模型的预测；
- \(f(\mathbf{x})\) 为真实数据生成函数；
- \(\sigma^2\) 为标签噪声的方差。

对于分类问题（例如使用合页损失），虽然形式化分解不同，但“偏差-方差权衡”的概念同样成立：更复杂的模型通常降低偏差但提高方差；更简单的模型则相反。

#### 10.2 在 SVM 中的体现（以软间隔与核 SVM 为例）

- 惩罚参数 \(C\)：
  - **\(C\) 大**（强烈惩罚误分类）：模型更注重拟合训练数据，通常**偏差下降、方差上升**，容易过拟合；
  - **\(C\) 小**（宽松惩罚）：模型更注重保持较大间隔，通常**偏差上升、方差下降**，容易欠拟合。
- RBF 核的带宽 \(\gamma\)：
  - **\(\gamma\) 大**（核函数更窄，决策边界更复杂、局部化）：**偏差下降、方差上升**；
  - **\(\gamma\) 小**（核函数更宽，决策边界更平滑）：**偏差上升、方差下降**。
- 多项式核的**次数 \(d\)** 与**系数 \(r\)**：
  - **次数 \(d\) 越大**，模型复杂度增加，**偏差下降、方差上升**；
  - 合理的 \(r\) 有助于稳定数值与控制复杂度。

几何直觉：
- **更复杂的边界（高 \(C\)、高 \(\gamma\)、高 \(d\)**）更贴近训练数据，间隔可能更小，泛化风险增大；
- **更简单的边界（低 \(C\)、低 \(\gamma\)、低 \(d\)**）更平滑，间隔更大，但可能无法捕捉数据的真实结构。

#### 10.3 调参与验证策略：平衡偏差与方差

- **标准化特征**：在 SVM 中，特征尺度直接影响间隔与核计算，务必进行标准化（如 Z-score）。
- **交叉验证（Cross-Validation）**：
  - 网格搜索/贝叶斯优化联合交叉验证，联合选择 \(C\)、核类型与核参数（如 \(\gamma\)、\(d\)、\(r\)）。
  - 量化时间序列应使用**时间序列交叉验证/滚动窗口（Walk-forward）**，避免未来数据泄露。
- **学习曲线（Learning Curve）**：通过样本规模-性能曲线，判断是高偏差（整体水平低、随样本增加提升有限）还是高方差（训练高、验证低、随样本增加逐步缩小差距）。
- **正则化与早停**：合理的 \(C\) 即为正则化强度；在迭代近似解（如 SGD）中可使用早停以抑制过拟合。

#### 10.4 边界值与异常场景（Boundary Cases & Pitfalls）

- **\(C \to \infty\)**：近似硬间隔，几乎不允许违例；若数据含噪或不可分，易过拟合，方差显著上升。
- **\(C \to 0\)**：几乎不惩罚误分类，模型过于简单，偏差显著上升，欠拟合。
- **\(\gamma \to \infty\)**（RBF 非常窄）：决策边界高度曲折，严重过拟合；
- **\(\gamma \to 0\)**（RBF 非常宽）：决策边界近似线性，可能欠拟合。
- **类别不平衡**：需结合 class_weight 或样本重采样，否则调参结论被主类主导，偏差-方差评估失真。
- **标签噪声/数据漂移**：
  - 标签噪声提高不可约误差，过高的 \(C\) 或 \(\gamma\) 会放大方差。
  - 市场状态漂移（Regime Shift）下，历史最优参数在未来失效，应做**滚动重训练**与**稳定性评估**。

#### 10.5 量化实务建议（Quant Practical Tips）

- 使用**滚动窗口交叉验证**选择 \(C\)、核类型与核参数，确保在不同市场阶段保持稳定表现；
- 对输入特征做**严谨的标准化与去噪**（异常值处理、稳健缩放），降低方差来源；
- 监控**样本外（Out-of-sample）**表现与**回测漂移**，避免将偏差-方差权衡的优化仅限定在样本内；
- 结合**特征选择/降维**（如 PCA、特征筛选）降低模型复杂度，提升偏差-方差的整体平衡；
- 对于高频或噪声强场景，适当降低 \(C\)、\(\gamma\)，提高模型鲁棒性；对于结构稳定的低频场景，可适度提高复杂度。

---

**总结：支持向量机是一种强大的分类和回归方法，通过最大化间隔和核技巧，可以处理线性和非线性问题。在量化交易中，SVM 可以用于涨跌预测、交易信号生成和市场状态识别。理解 SVM 的原理、核函数的选择和参数调优，对于构建有效的量化模型至关重要。通过合理的数据预处理、参数调优和模型验证，可以充分发挥 SVM 的优势，提高交易策略的准确性和稳定性。**

