# Shrinkage 回归：Ridge、Lasso 与 Elastic Net

## 目录
1. [Shrinkage 回归概述](#一-shrinkage-回归概述)
2. [Ridge 回归](#二-ridge-回归)
3. [Lasso 回归](#三-lasso-回归)
4. [Elastic Net](#四-elastic-net)
5. [三种方法的对比](#五-三种方法的对比)
6. [正则化路径](#六-正则化路径)
7. [超参数选择](#七-超参数选择)
8. [在量化交易中的应用](#八-在量化交易中的应用)
9. [Python 实现示例](#九-python-实现示例)
10. [总结](#十-总结)

---

## 一、Shrinkage 回归概述

### 1.1 什么是 Shrinkage 回归

**Shrinkage 回归（收缩回归）**是一类通过添加正则化项来约束模型参数的方法，也称为**正则化回归**。

**核心思想：**
- 在损失函数中添加**惩罚项**（Penalty Term）
- 通过**收缩**（Shrinkage）参数值，减少模型复杂度
- 在偏差和方差之间取得平衡，提高泛化能力

### 1.2 为什么需要 Shrinkage 回归

**普通最小二乘法（OLS）的问题：**

1. **过拟合**
   - 当特征数量 $p$ 接近或超过样本数量 $n$ 时，容易过拟合
   - 模型在训练集上表现好，但泛化能力差

2. **多重共线性**
   - 特征之间高度相关时，OLS 估计不稳定
   - 系数方差大，对数据扰动敏感

3. **高方差**
   - 当 $p$ 很大时，模型方差高
   - 预测不稳定

**Shrinkage 回归的解决方案：**
- 通过正则化约束参数，减少模型复杂度
- 降低方差，提高泛化能力
- 处理多重共线性问题

### 1.3 Shrinkage 回归的类型

**主要类型：**

1. **Ridge 回归（L2 正则化）**
   - 惩罚参数的平方和
   - 参数趋向于小值，但不为 0
   - 适用于多重共线性问题

2. **Lasso 回归（L1 正则化）**
   - 惩罚参数的绝对值之和
   - 可以将参数压缩到 0，实现特征选择
   - 适用于高维数据和特征选择

3. **Elastic Net（L1 + L2 正则化）**
   - 结合 L1 和 L2 正则化
   - 兼具两种方法的优点
   - 适用于特征数量大于样本数量的情况

### 1.4 正则化的数学形式

**一般形式：**

$$
\min_{\boldsymbol{\beta}} \left\{ \frac{1}{2n}\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda P(\boldsymbol{\beta}) \right\}
$$

其中：
- $\frac{1}{2n}\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2$：损失函数（平方误差）
- $\lambda P(\boldsymbol{\beta})$：正则化项（惩罚项）
- $\lambda \geq 0$：正则化参数（控制惩罚强度）

**不同正则化项：**

- **Ridge（L2）**：$P(\boldsymbol{\beta}) = \frac{1}{2}\|\boldsymbol{\beta}\|_2^2 = \frac{1}{2}\sum_{j=1}^{p}\beta_j^2$
- **Lasso（L1）**：$P(\boldsymbol{\beta}) = \|\boldsymbol{\beta}\|_1 = \sum_{j=1}^{p}|\beta_j|$
- **Elastic Net**：$P(\boldsymbol{\beta}) = \alpha\|\boldsymbol{\beta}\|_1 + (1-\alpha)\frac{1}{2}\|\boldsymbol{\beta}\|_2^2$

---

## 二、Ridge 回归

### 2.1 Ridge 回归概述

**Ridge 回归（岭回归）**由 Hoerl 和 Kennard 在 1970 年提出，是最早的正则化回归方法。

**核心特点：**
- 使用 L2 正则化（参数的平方和）
- 参数趋向于小值，但不会变为 0
- 适用于多重共线性问题

### 2.2 Ridge 回归的目标函数

**目标函数：**

$$
\min_{\boldsymbol{\beta}} \left\{ \frac{1}{2n}\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda\|\boldsymbol{\beta}\|_2^2 \right\}
$$

或者：

$$
\min_{\boldsymbol{\beta}} \left\{ \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda\sum_{j=1}^{p}\beta_j^2 \right\}
$$

其中：
- $\lambda \geq 0$：正则化参数
- $\lambda = 0$ 时，退化为普通最小二乘法
- $\lambda \to \infty$ 时，所有参数趋向于 0

### 2.3 Ridge 回归的参数估计

**解析解：**

对目标函数求导并令其等于 0：

$$
\frac{\partial}{\partial \boldsymbol{\beta}} \left\{ \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda\|\boldsymbol{\beta}\|^2 \right\} = 0
$$

得到：

$$
-2\mathbf{X}'(\mathbf{y} - \mathbf{X}\boldsymbol{\beta}) + 2\lambda\boldsymbol{\beta} = 0
$$

整理得到：

$$
(\mathbf{X}'\mathbf{X} + \lambda\mathbf{I})\boldsymbol{\beta} = \mathbf{X}'\mathbf{y}
$$

**Ridge 估计量：**

$$
\hat{\boldsymbol{\beta}}_{\text{Ridge}} = (\mathbf{X}'\mathbf{X} + \lambda\mathbf{I})^{-1}\mathbf{X}'\mathbf{y}
$$

**关键性质：**
- 即使 $\mathbf{X}'\mathbf{X}$ 不可逆，$(\mathbf{X}'\mathbf{X} + \lambda\mathbf{I})$ 也是可逆的
- 解决了多重共线性问题
- 参数估计是唯一的

### 2.4 Ridge 回归的几何解释

**约束优化视角：**

Ridge 回归等价于约束优化问题：

$$
\min_{\boldsymbol{\beta}} \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 \quad \text{s.t.} \quad \|\boldsymbol{\beta}\|^2 \leq t
$$

其中 $t$ 是与 $\lambda$ 对应的约束半径。

**几何意义：**
- 在参数空间中，约束是一个**球面**（L2 球）
- 最优解在约束球面与损失函数等高线的切点
- 参数被"拉向"原点，但不会变为 0

### 2.5 Ridge 回归的性质

**1. 偏差-方差权衡**

**偏差：**
- Ridge 估计是有偏的（除非 $\lambda = 0$）
- 偏差随 $\lambda$ 增大而增大

**方差：**
- Ridge 估计的方差小于 OLS 估计
- 方差随 $\lambda$ 增大而减小

**均方误差（MSE）：**
$$
\text{MSE} = \text{Bias}^2 + \text{Var}
$$

当 $\lambda$ 适当时，MSE 可能小于 OLS。

**2. 参数收缩**

Ridge 回归将参数向 0 收缩，但不会完全为 0：

$$
|\hat{\beta}_j^{\text{Ridge}}| < |\hat{\beta}_j^{\text{OLS}}|
$$

**3. 尺度敏感性**

Ridge 回归对特征的尺度敏感，因此需要标准化：

$$
\tilde{X}_{ij} = \frac{X_{ij} - \bar{X}_j}{s_j}
$$

其中 $s_j$ 是第 $j$ 个特征的标准差。

### 2.6 Ridge 回归的优缺点

**优点：**
- ✅ 解决多重共线性问题
- ✅ 降低模型方差
- ✅ 有解析解，计算高效
- ✅ 参数估计稳定

**缺点：**
- ❌ 不能进行特征选择（参数不为 0）
- ❌ 需要标准化
- ❌ 需要选择正则化参数 $\lambda$

---

## 三、Lasso 回归

### 3.1 Lasso 回归概述

**Lasso（Least Absolute Shrinkage and Selection Operator）**由 Tibshirani 在 1996 年提出。

**核心特点：**
- 使用 L1 正则化（参数的绝对值之和）
- 可以将参数压缩到 0，实现**特征选择**
- 适用于高维数据和稀疏模型

### 3.2 Lasso 回归的目标函数

**目标函数：**

$$
\min_{\boldsymbol{\beta}} \left\{ \frac{1}{2n}\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda\|\boldsymbol{\beta}\|_1 \right\}
$$

或者：

$$
\min_{\boldsymbol{\beta}} \left\{ \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda\sum_{j=1}^{p}|\beta_j| \right\}
$$

其中：
- $\lambda \geq 0$：正则化参数
- $\lambda = 0$ 时，退化为普通最小二乘法
- $\lambda$ 足够大时，所有参数变为 0

### 3.3 Lasso 回归的参数估计

**Lasso 没有解析解**，需要使用优化算法求解。

**优化方法：**

1. **坐标下降法（Coordinate Descent）**
   - 每次优化一个参数
   - 其他参数固定
   - 迭代直到收敛

2. **最小角回归（LARS）**
   - 专门为 Lasso 设计的算法
   - 可以计算整个正则化路径

3. **梯度下降法**
   - 使用次梯度（Subgradient）
   - 因为绝对值函数在 0 处不可导

### 3.4 Lasso 回归的几何解释

**约束优化视角：**

Lasso 回归等价于约束优化问题：

$$
\min_{\boldsymbol{\beta}} \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 \quad \text{s.t.} \quad \|\boldsymbol{\beta}\|_1 \leq t
$$

其中 $t$ 是与 $\lambda$ 对应的约束半径。

**几何意义：**
- 在参数空间中，约束是一个**菱形**（L1 球）
- 最优解可能在约束的"角"上（某些参数为 0）
- 实现了特征选择

### 3.5 Lasso 回归的性质

**1. 特征选择**

Lasso 可以将不重要的特征的系数压缩到 0：

$$
\hat{\beta}_j^{\text{Lasso}} = 0 \quad \text{（对于某些 } j \text{）}
$$

**2. 稀疏性**

Lasso 倾向于产生**稀疏模型**（参数中很多为 0），这在以下情况特别有用：
- 特征数量 $p$ 很大
- 只有少数特征真正重要
- 需要模型解释性

**3. 参数收缩**

对于非零参数，Lasso 也会将其向 0 收缩：

$$
|\hat{\beta}_j^{\text{Lasso}}| \leq |\hat{\beta}_j^{\text{OLS}}|
$$

### 3.6 Lasso 回归的局限性

**1. 特征选择的不稳定性**

当特征高度相关时：
- Lasso 可能随机选择其中一个特征
- 结果不稳定
- 可能丢失重要信息

**2. 最多选择 $n$ 个特征**

当 $p > n$ 时：
- Lasso 最多只能选择 $n$ 个非零参数
- 可能无法选择所有重要特征

**3. 计算复杂度**

- 没有解析解
- 需要迭代优化
- 计算时间较长

---

## 四、Elastic Net

### 4.1 Elastic Net 概述

**Elastic Net**由 Zou 和 Hastie 在 2005 年提出，结合了 L1 和 L2 正则化。

**核心特点：**
- 结合 Ridge 和 Lasso 的优点
- 适用于 $p > n$ 的情况
- 对相关特征更稳定

### 4.2 Elastic Net 的目标函数

**目标函数：**

$$
\min_{\boldsymbol{\beta}} \left\{ \frac{1}{2n}\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda\left[\alpha\|\boldsymbol{\beta}\|_1 + (1-\alpha)\frac{1}{2}\|\boldsymbol{\beta}\|_2^2\right] \right\}
$$

其中：
- $\lambda \geq 0$：正则化参数（控制整体惩罚强度）
- $\alpha \in [0, 1]$：混合参数
  - $\alpha = 1$：Lasso 回归
  - $\alpha = 0$：Ridge 回归
  - $0 < \alpha < 1$：Elastic Net

### 4.3 Elastic Net 的优势

**1. 结合两种方法的优点**

- **L1 部分**：实现特征选择（稀疏性）
- **L2 部分**：处理多重共线性（稳定性）

**2. 对相关特征更稳定**

- 当特征高度相关时，Elastic Net 倾向于同时选择或排除它们
- 比 Lasso 更稳定

**3. 适用于 $p > n$**

- 可以处理特征数量大于样本数量的情况
- 最多可以选择 $n$ 个以上的特征

### 4.4 Elastic Net 的几何解释

**约束优化视角：**

Elastic Net 等价于约束优化问题：

$$
\min_{\boldsymbol{\beta}} \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 \quad \text{s.t.} \quad \alpha\|\boldsymbol{\beta}\|_1 + (1-\alpha)\frac{1}{2}\|\boldsymbol{\beta}\|_2^2 \leq t
$$

**几何意义：**
- 约束区域是 L1 球和 L2 球的凸组合
- 结合了菱形的"角"和球面的"平滑"
- 既可以选择特征，又保持稳定性

---

## 五、三种方法的对比

### 5.1 方法对比表

| 特性 | Ridge | Lasso | Elastic Net |
|------|-------|-------|-------------|
| **正则化项** | L2 ($\|\boldsymbol{\beta}\|_2^2$) | L1 ($\|\boldsymbol{\beta}\|_1$) | L1 + L2 |
| **特征选择** | ❌ | ✅ | ✅ |
| **参数为 0** | 不会 | 可以 | 可以 |
| **多重共线性** | 处理良好 | 不稳定 | 处理良好 |
| **解析解** | ✅ | ❌ | ❌ |
| **计算速度** | 快 | 中等 | 中等 |
| **适用场景** | 多重共线性 | 特征选择 | 综合场景 |
| **$p > n$** | 可以 | 最多选 $n$ 个 | 可以 |

### 5.2 选择建议

**选择 Ridge 当：**
- 所有特征都可能重要
- 存在多重共线性
- 不需要特征选择
- 需要快速计算

**选择 Lasso 当：**
- 需要特征选择
- 特征数量 $p$ 很大
- 只有少数特征重要
- 需要稀疏模型

**选择 Elastic Net 当：**
- 特征高度相关
- 需要特征选择但也要稳定性
- $p > n$ 且需要选择多个特征
- 不确定选择哪种方法时

---

## 六、正则化路径

### 6.1 什么是正则化路径

**正则化路径（Regularization Path）**是指随着正则化参数 $\lambda$ 的变化，参数估计值的变化轨迹。

### 6.2 Ridge 回归的正则化路径

**Ridge 路径：**
- 当 $\lambda = 0$ 时，参数等于 OLS 估计
- 当 $\lambda$ 增大时，所有参数平滑地趋向于 0
- 参数不会突然变为 0

**可视化：**
- 横轴：$\lambda$（或 $\log(\lambda)$）
- 纵轴：参数值 $\beta_j$
- 每条线代表一个参数

### 6.3 Lasso 回归的正则化路径

**Lasso 路径：**
- 当 $\lambda = 0$ 时，参数等于 OLS 估计
- 当 $\lambda$ 增大时，参数逐渐向 0 收缩
- 某些参数会在某个 $\lambda$ 值处突然变为 0
- 体现了特征选择的过程

**特点：**
- 路径是分段线性的
- 参数可能突然变为 0
- 可以清楚地看到特征选择的顺序

### 6.4 Elastic Net 的正则化路径

**Elastic Net 路径：**
- 结合了 Ridge 和 Lasso 的特点
- 参数变化相对平滑
- 但仍然可以实现特征选择

---

## 七、超参数选择

### 7.1 交叉验证

**K 折交叉验证：**

1. 将数据分为 $K$ 折
2. 对每个 $\lambda$ 值：
   - 用 $K-1$ 折训练模型
   - 用剩余 1 折验证
   - 计算平均验证误差
3. 选择验证误差最小的 $\lambda$

**留一交叉验证（LOOCV）：**
- 特殊情况的 K 折交叉验证（$K = n$）
- 计算量大，但无偏

### 7.2 信息准则

**AIC（Akaike Information Criterion）：**

$$
\text{AIC} = 2k - 2\ln(L)
$$

其中 $k$ 是参数数量，$L$ 是似然函数值。

**BIC（Bayesian Information Criterion）：**

$$
\text{BIC} = k\ln(n) - 2\ln(L)
$$

BIC 对模型复杂度惩罚更重。

### 7.3 正则化参数的选择范围

**常用范围：**
- $\lambda \in [10^{-4}, 10^4]$（对数尺度）
- 使用网格搜索或随机搜索
- 通常在对数尺度上均匀采样

**对于 Elastic Net：**
- $\lambda \in [10^{-4}, 10^4]$
- $\alpha \in [0, 1]$（通常取 0.1, 0.5, 0.7, 0.9, 1.0）

---

## 八、在量化交易中的应用

### 8.1 因子模型

**使用 Ridge 回归构建因子模型：**

**应用：**
- Fama-French 多因子模型
- 处理因子之间的多重共线性
- 稳定估计因子暴露

**优势：**
- 处理因子相关性
- 提高模型稳定性
- 减少过拟合

### 8.2 特征选择

**使用 Lasso 回归进行特征选择：**

**应用：**
- 从大量技术指标中选择重要特征
- 识别有效的预测因子
- 构建稀疏的预测模型

**优势：**
- 自动特征选择
- 提高模型可解释性
- 减少过拟合风险

### 8.3 高维数据建模

**使用 Elastic Net 处理高维数据：**

**场景：**
- 特征数量远大于样本数量（$p >> n$）
- 需要同时进行特征选择和稳定性处理
- 因子挖掘和选择

**优势：**
- 处理高维数据
- 特征选择 + 稳定性
- 适用于量化因子研究

### 8.4 风险模型

**使用 Shrinkage 回归构建风险模型：**

**应用：**
- 估计资产的风险暴露
- 构建风险因子模型
- 投资组合风险分解

---

## 九、Python 实现示例

### 9.1 Ridge 回归实现

```python
from sklearn.linear_model import Ridge, RidgeCV
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score
import numpy as np
import pandas as pd

# 数据标准化（重要！）
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Ridge 回归
ridge = Ridge(alpha=1.0)  # alpha 是正则化参数 lambda
ridge.fit(X_train_scaled, y_train)

# 预测
y_pred = ridge.predict(X_test_scaled)

# 使用交叉验证选择最优 alpha
alphas = np.logspace(-4, 4, 100)  # 10^-4 到 10^4
ridge_cv = RidgeCV(alphas=alphas, cv=5, scoring='neg_mean_squared_error')
ridge_cv.fit(X_train_scaled, y_train)

print(f"最优 alpha: {ridge_cv.alpha_}")
print(f"交叉验证分数: {-ridge_cv.best_score_:.4f}")

# 系数
coefficients = pd.DataFrame({
    'feature': feature_names,
    'coefficient': ridge_cv.coef_
}).sort_values('coefficient', key=abs, ascending=False)

print("\n系数:")
print(coefficients)
```

### 9.2 Lasso 回归实现

```python
from sklearn.linear_model import Lasso, LassoCV
import matplotlib.pyplot as plt

# Lasso 回归
lasso = Lasso(alpha=0.1)
lasso.fit(X_train_scaled, y_train)

# 预测
y_pred = lasso.predict(X_test_scaled)

# 使用交叉验证选择最优 alpha
alphas = np.logspace(-4, 1, 100)
lasso_cv = LassoCV(alphas=alphas, cv=5, random_state=42, max_iter=1000)
lasso_cv.fit(X_train_scaled, y_train)

print(f"最优 alpha: {lasso_cv.alpha_}")
print(f"选择的特征数: {np.sum(lasso_cv.coef_ != 0)}")

# 系数
coefficients = pd.DataFrame({
    'feature': feature_names,
    'coefficient': lasso_cv.coef_
})
coefficients = coefficients[coefficients['coefficient'] != 0].sort_values(
    'coefficient', key=abs, ascending=False
)

print("\n选择的特征和系数:")
print(coefficients)
```

### 9.3 Elastic Net 实现

```python
from sklearn.linear_model import ElasticNet, ElasticNetCV

# Elastic Net
elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)  # l1_ratio 是 alpha 参数
elastic_net.fit(X_train_scaled, y_train)

# 使用交叉验证选择最优参数
alphas = np.logspace(-4, 1, 50)
l1_ratios = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0]

elastic_net_cv = ElasticNetCV(
    alphas=alphas,
    l1_ratio=l1_ratios,
    cv=5,
    random_state=42,
    max_iter=1000
)
elastic_net_cv.fit(X_train_scaled, y_train)

print(f"最优 alpha: {elastic_net_cv.alpha_}")
print(f"最优 l1_ratio: {elastic_net_cv.l1_ratio_}")
print(f"选择的特征数: {np.sum(elastic_net_cv.coef_ != 0)}")
```

### 9.4 正则化路径可视化

```python
from sklearn.linear_model import lasso_path, enet_path

# Lasso 路径
alphas_lasso, coefs_lasso, _ = lasso_path(
    X_train_scaled, y_train,
    alphas=alphas,
    max_iter=1000
)

# Elastic Net 路径
alphas_enet, coefs_enet, _ = enet_path(
    X_train_scaled, y_train,
    alphas=alphas,
    l1_ratio=0.5,
    max_iter=1000
)

# 绘制正则化路径
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(np.log10(alphas_lasso), coefs_lasso.T)
plt.xlabel('log10(alpha)')
plt.ylabel('系数')
plt.title('Lasso 正则化路径')
plt.axvline(x=np.log10(lasso_cv.alpha_), color='r', linestyle='--', label='最优 alpha')
plt.legend()
plt.grid(True)

plt.subplot(1, 2, 2)
plt.plot(np.log10(alphas_enet), coefs_enet.T)
plt.xlabel('log10(alpha)')
plt.ylabel('系数')
plt.title('Elastic Net 正则化路径')
plt.axvline(x=np.log10(elastic_net_cv.alpha_), color='r', linestyle='--', label='最优 alpha')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
```

### 9.5 量化交易应用示例

```python
import yfinance as yf
import pandas as pd
import numpy as np
from sklearn.linear_model import LassoCV, RidgeCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score

def get_stock_data(symbol, period='2y'):
    """获取股票数据"""
    stock = yf.Ticker(symbol)
    data = stock.history(period=period)
    return data

def calculate_features(data):
    """计算技术指标特征"""
    df = data.copy()
    
    # 收益率
    df['Return'] = df['Close'].pct_change()
    
    # 技术指标
    # RSI
    delta = df['Close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df['RSI'] = 100 - (100 / (1 + rs))
    
    # MACD
    exp1 = df['Close'].ewm(span=12, adjust=False).mean()
    exp2 = df['Close'].ewm(span=26, adjust=False).mean()
    df['MACD'] = exp1 - exp2
    
    # 均线
    for window in [5, 10, 20, 50]:
        df[f'MA_{window}'] = df['Close'].rolling(window=window).mean()
        df[f'MA_{window}_ratio'] = df['Close'] / df[f'MA_{window}']
    
    # 波动率
    for window in [5, 10, 20]:
        df[f'Volatility_{window}'] = df['Return'].rolling(window=window).std()
    
    # 成交量
    df['Volume_MA'] = df['Volume'].rolling(window=20).mean()
    df['Volume_Ratio'] = df['Volume'] / df['Volume_MA']
    
    # 动量
    for period in [5, 10, 20]:
        df[f'Momentum_{period}'] = df['Close'].pct_change(period)
    
    return df

def main():
    # 获取数据
    data = get_stock_data('AAPL', period='3y')
    
    # 计算特征
    data = calculate_features(data)
    
    # 创建目标变量（未来收益率）
    data['Future_Return'] = data['Return'].shift(-1)
    
    # 选择特征
    feature_cols = [col for col in data.columns 
                   if col not in ['Future_Return', 'Return', 'Close', 'High', 'Low', 'Open', 'Volume']]
    
    # 删除缺失值
    data = data[feature_cols + ['Future_Return']].dropna()
    
    X = data[feature_cols].values
    y = data['Future_Return'].values
    
    # 时间序列划分
    split_idx = int(len(X) * 0.8)
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    # 标准化
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Lasso 回归（特征选择）
    print("=" * 50)
    print("Lasso 回归")
    print("=" * 50)
    
    alphas = np.logspace(-4, 1, 100)
    lasso_cv = LassoCV(alphas=alphas, cv=5, random_state=42, max_iter=1000)
    lasso_cv.fit(X_train_scaled, y_train)
    
    y_pred_lasso = lasso_cv.predict(X_test_scaled)
    mse_lasso = mean_squared_error(y_test, y_pred_lasso)
    r2_lasso = r2_score(y_test, y_pred_lasso)
    
    print(f"最优 alpha: {lasso_cv.alpha_:.6f}")
    print(f"选择的特征数: {np.sum(lasso_cv.coef_ != 0)} / {len(feature_cols)}")
    print(f"测试集 MSE: {mse_lasso:.6f}")
    print(f"测试集 R²: {r2_lasso:.4f}")
    
    # 选择的特征
    selected_features = pd.DataFrame({
        'feature': feature_cols,
        'coefficient': lasso_cv.coef_
    })
    selected_features = selected_features[selected_features['coefficient'] != 0].sort_values(
        'coefficient', key=abs, ascending=False
    )
    
    print("\n选择的特征:")
    print(selected_features)
    
    # Ridge 回归（稳定性）
    print("\n" + "=" * 50)
    print("Ridge 回归")
    print("=" * 50)
    
    alphas_ridge = np.logspace(-4, 4, 100)
    ridge_cv = RidgeCV(alphas=alphas_ridge, cv=5, scoring='neg_mean_squared_error')
    ridge_cv.fit(X_train_scaled, y_train)
    
    y_pred_ridge = ridge_cv.predict(X_test_scaled)
    mse_ridge = mean_squared_error(y_test, y_pred_ridge)
    r2_ridge = r2_score(y_test, y_pred_ridge)
    
    print(f"最优 alpha: {ridge_cv.alpha_:.6f}")
    print(f"测试集 MSE: {mse_ridge:.6f}")
    print(f"测试集 R²: {r2_ridge:.4f}")
    
    return lasso_cv, ridge_cv, selected_features

if __name__ == '__main__':
    lasso_model, ridge_model, features = main()
```

---

## 十、总结

### 10.1 核心要点

**1. Shrinkage 回归的本质**
- 通过添加正则化项约束模型参数
- 在偏差和方差之间取得平衡
- 提高模型的泛化能力

**2. 三种方法的特点**
- **Ridge**：L2 正则化，处理多重共线性，参数不为 0
- **Lasso**：L1 正则化，特征选择，参数可以为 0
- **Elastic Net**：L1 + L2，结合两种方法的优点

**3. 应用场景**
- 多重共线性问题 → Ridge
- 特征选择 → Lasso
- 综合场景 → Elastic Net

### 10.2 优势与局限

**优势：**
- ✅ 减少过拟合
- ✅ 处理多重共线性
- ✅ 提高模型稳定性
- ✅ Lasso 可以实现特征选择

**局限：**
- ❌ 需要选择正则化参数
- ❌ 需要数据标准化
- ❌ Lasso 对相关特征不稳定

### 10.3 在量化交易中的应用

**主要应用：**
1. **因子模型**：处理因子多重共线性
2. **特征选择**：从大量指标中选择重要特征
3. **高维建模**：处理 $p > n$ 的情况
4. **风险模型**：构建稳定的风险因子模型

**注意事项：**
- 必须进行特征标准化
- 使用交叉验证选择正则化参数
- 注意时间序列数据的特殊性
- 避免数据泄露

---

**总结：Shrinkage 回归是一类重要的正则化方法，通过约束模型参数来提高泛化能力。Ridge、Lasso 和 Elastic Net 各有特点，适用于不同的场景。在量化交易中，这些方法被广泛应用于因子模型、特征选择和风险建模。理解这些方法的原理、特点和应用，对于构建有效的量化模型至关重要。**

