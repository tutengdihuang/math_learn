# Lasso 回归：原理、算法与应用

## 目录
1. [Lasso 回归概述](#一-lasso-回归概述)
2. [Lasso 回归的数学原理](#二-lasso-回归的数学原理)
3. [Lasso 回归的优化算法](#三-lasso-回归的优化算法)
4. [Lasso 回归的性质](#四-lasso-回归的性质)
5. [特征选择机制](#五-特征选择机制)
6. [正则化路径](#六-正则化路径)
7. [Lasso 的变体](#七-lasso-的变体)
8. [超参数选择](#八-超参数选择)
9. [在量化交易中的应用](#九-在量化交易中的应用)
10. [Python 实现示例](#十-python-实现示例)
11. [总结](#十一-总结)

---

## 一、Lasso 回归概述

### 1.1 什么是 Lasso 回归

**Lasso（Least Absolute Shrinkage and Selection Operator）**是 Tibshirani 在 1996 年提出的一种正则化回归方法。

**核心特点：**
- 使用 **L1 正则化**（参数的绝对值之和）
- 可以将参数**压缩到 0**，实现**特征选择**
- 产生**稀疏模型**（参数中很多为 0）
- 适用于高维数据和特征选择

### 1.2 Lasso 回归的动机

**问题背景：**

1. **高维数据**
   - 特征数量 $p$ 很大，甚至 $p > n$（样本数量）
   - 需要从大量特征中选择重要特征

2. **特征选择**
   - 只有少数特征真正重要
   - 需要自动识别和选择这些特征

3. **模型解释性**
   - 稀疏模型更容易解释
   - 只关注重要特征

4. **过拟合控制**
   - 通过特征选择减少模型复杂度
   - 提高泛化能力

### 1.3 Lasso 回归的优势

**主要优势：**
- ✅ **自动特征选择**：将不重要特征的系数压缩到 0
- ✅ **稀疏性**：产生稀疏模型，易于解释
- ✅ **处理高维数据**：适用于 $p > n$ 的情况
- ✅ **计算效率**：有高效的优化算法

### 1.4 Lasso 回归的应用场景

**主要应用：**
1. **特征选择**：从大量特征中选择重要特征
2. **高维回归**：$p > n$ 的情况
3. **稀疏建模**：需要稀疏模型的场景
4. **模型解释**：需要可解释的模型
5. **量化交易**：因子选择、特征工程

---

## 二、Lasso 回归的数学原理

### 2.1 Lasso 回归的目标函数

**标准形式：**

$$
\min_{\boldsymbol{\beta}} \left\{ \frac{1}{2n}\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda\|\boldsymbol{\beta}\|_1 \right\}
$$

其中：
- $\mathbf{y} \in \mathbb{R}^n$：响应变量向量
- $\mathbf{X} \in \mathbb{R}^{n \times p}$：设计矩阵
- $\boldsymbol{\beta} \in \mathbb{R}^p$：参数向量
- $\lambda \geq 0$：正则化参数
- $\|\boldsymbol{\beta}\|_1 = \sum_{j=1}^{p}|\beta_j|$：L1 范数

**等价形式：**

$$
\min_{\boldsymbol{\beta}} \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 \quad \text{s.t.} \quad \|\boldsymbol{\beta}\|_1 \leq t
$$

其中 $t$ 是与 $\lambda$ 对应的约束半径。

### 2.2 Lasso 回归的几何解释

**约束优化视角：**

Lasso 回归等价于在 L1 球约束下最小化平方误差：

$$
\min_{\boldsymbol{\beta}} \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 \quad \text{s.t.} \quad \sum_{j=1}^{p}|\beta_j| \leq t
$$

**几何意义：**

在二维情况下（$p = 2$）：
- 约束区域是一个**菱形**（L1 球）：$|\beta_1| + |beta_2| \leq t$
- 损失函数的等高线是**椭圆**
- 最优解在约束菱形的"角"上（某些参数为 0）

**为什么会产生稀疏性：**

- L1 球的"角"在坐标轴上
- 当最优解在"角"上时，对应的参数为 0
- 这解释了为什么 Lasso 可以实现特征选择

### 2.3 Lasso 回归的 KKT 条件

**拉格朗日函数：**

$$
L(\boldsymbol{\beta}, \lambda) = \frac{1}{2n}\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda\|\boldsymbol{\beta}\|_1
$$

**KKT 条件：**

对于每个 $j = 1, 2, \ldots, p$：

$$
\begin{cases}
-\frac{1}{n}\mathbf{x}_j'(\mathbf{y} - \mathbf{X}\boldsymbol{\beta}) + \lambda \text{sign}(\beta_j) = 0 & \text{if } \beta_j \neq 0 \\
\left|-\frac{1}{n}\mathbf{x}_j'(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})\right| \leq \lambda & \text{if } \beta_j = 0
\end{cases}
$$

**解释：**
- 如果 $|\mathbf{x}_j'(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})| < n\lambda$，则 $\beta_j = 0$
- 这给出了特征被选择的阈值条件

### 2.4 Lasso 回归的软阈值算子

**软阈值算子（Soft Thresholding）：**

对于标量 $z$ 和阈值 $\lambda$，软阈值算子定义为：

$$
S_\lambda(z) = \text{sign}(z)(|z| - \lambda)_+ = \begin{cases}
z - \lambda & \text{if } z > \lambda \\
0 & \text{if } |z| \leq \lambda \\
z + \lambda & \text{if } z < -\lambda
\end{cases}
$$

**Lasso 的单变量解：**

对于单变量 Lasso 问题：

$$
\min_{\beta} \frac{1}{2}(z - \beta)^2 + \lambda|\beta|
$$

最优解为：

$$
\hat{\beta} = S_\lambda(z)
$$

**几何解释：**
- 如果 $|z| \leq \lambda$，则 $\hat{\beta} = 0$（被压缩到 0）
- 如果 $|z| > \lambda$，则 $\hat{\beta} = z - \lambda \text{sign}(z)$（向 0 收缩）

---

## 三、Lasso 回归的优化算法

### 3.1 坐标下降法（Coordinate Descent）

**算法思想：**
- 每次只优化一个参数 $\beta_j$
- 其他参数固定
- 迭代直到收敛

**算法流程：**

```
1. 初始化：$\boldsymbol{\beta}^{(0)} = \mathbf{0}$

2. 对于 $k = 1, 2, \ldots$：
   对于 $j = 1, 2, \ldots, p$：
      a. 计算残差：$\mathbf{r} = \mathbf{y} - \sum_{l \neq j} \mathbf{x}_l \beta_l^{(k)}$
      b. 计算：$z_j = \frac{1}{n}\mathbf{x}_j'\mathbf{r}$
      c. 更新：$\beta_j^{(k)} = S_\lambda(z_j)$

3. 直到收敛
```

**优点：**
- 简单高效
- 每次更新只需要计算一个特征的相关系数
- 收敛速度快

### 3.2 最小角回归（LARS）

**LARS（Least Angle Regression）**是专门为 Lasso 设计的算法。

**算法思想：**
- 从全零模型开始
- 逐步添加特征
- 沿着"最小角"方向移动

**LARS-Lasso 算法：**

```
1. 初始化：$\boldsymbol{\beta} = \mathbf{0}$，活跃集 $A = \emptyset$

2. 计算当前残差：$\mathbf{r} = \mathbf{y} - \mathbf{X}\boldsymbol{\beta}$

3. 找到与残差最相关的特征：$j^* = \arg\max_j |\mathbf{x}_j'\mathbf{r}|$

4. 如果 $|\mathbf{x}_{j^*}'\mathbf{r}| < \lambda$，停止

5. 将 $j^*$ 加入活跃集 $A$

6. 沿着等角方向移动，直到：
   - 某个特征的相关系数等于 $\lambda$
   - 某个特征的系数变为 0

7. 重复步骤 2-6
```

**优点：**
- 可以计算整个正则化路径
- 计算效率高
- 理论性质好

### 3.3 次梯度方法

**次梯度（Subgradient）：**

由于绝对值函数在 0 处不可导，需要使用次梯度。

**L1 范数的次梯度：**

$$
\partial|\beta_j| = \begin{cases}
\{1\} & \text{if } \beta_j > 0 \\
[-1, 1] & \text{if } \beta_j = 0 \\
\{-1\} & \text{if } \beta_j < 0
\end{cases}
$$

**次梯度下降：**

$$
\boldsymbol{\beta}^{(k+1)} = \boldsymbol{\beta}^{(k)} - \eta_k \mathbf{g}^{(k)}
$$

其中 $\mathbf{g}^{(k)}$ 是次梯度。

### 3.4 算法对比

| 算法 | 优点 | 缺点 | 适用场景 |
|------|------|------|----------|
| **坐标下降** | 简单、高效 | 可能收敛慢 | 一般情况 |
| **LARS** | 计算路径、理论好 | 实现复杂 | 需要路径 |
| **次梯度** | 通用 | 收敛慢 | 特殊情况 |

---

## 四、Lasso 回归的性质

### 4.1 稀疏性

**定义：** 如果参数向量 $\boldsymbol{\beta}$ 中很多元素为 0，则称其为稀疏的。

**Lasso 的稀疏性：**
- Lasso 倾向于产生稀疏解
- 当 $\lambda$ 足够大时，所有参数都为 0
- 当 $\lambda$ 适当时，只有部分参数为 0

**稀疏性的优势：**
- 模型更简单
- 计算更快
- 更容易解释

### 4.2 特征选择的一致性

**一致性条件：**

在一定的条件下，Lasso 可以一致地选择真实模型中的特征。

**必要条件（Irrepresentable Condition）：**

设真实模型为 $S = \{j : \beta_j^* \neq 0\}$，则：

$$
\max_{j \notin S} \|\mathbf{X}_S' \mathbf{X}_j (\mathbf{X}_S' \mathbf{X}_S)^{-1}\|_1 < 1
$$

其中 $\mathbf{X}_S$ 是活跃特征的设计矩阵。

**解释：**
- 非活跃特征与活跃特征的相关性不能太强
- 如果条件不满足，Lasso 可能选择错误的特征

### 4.3 参数估计的一致性

**一致性：**

在一定的正则条件下，Lasso 估计是一致的：

$$
\|\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}^*\|_2 = O_p\left(\sqrt{\frac{\log p}{n}}\right)
$$

**条件：**
- 需要选择合适的 $\lambda$
- 需要满足一定的正则条件

### 4.4 最多选择 $n$ 个特征

**重要性质：**

当 $p > n$ 时，Lasso 最多只能选择 $n$ 个非零参数。

**原因：**
- 当 $n$ 个特征被选择后，残差与所有特征正交
- 无法再选择新特征

**影响：**
- 如果真实模型有超过 $n$ 个重要特征，Lasso 可能无法全部选择
- 这是 Lasso 的一个局限性

---

## 五、特征选择机制

### 5.1 为什么 Lasso 能选择特征

**几何解释：**

1. **L1 球的"角"在坐标轴上**
   - 当最优解在"角"上时，对应参数为 0
   - 这自然实现了特征选择

2. **软阈值效应**
   - 如果特征的相关系数 $|z_j| < \lambda$，则 $\beta_j = 0$
   - 只有"足够强"的特征才会被选择

### 5.2 特征选择的顺序

**Lasso 路径：**

随着 $\lambda$ 从大到小变化：
1. 首先选择与响应变量最相关的特征
2. 然后逐步添加其他特征
3. 特征选择的顺序反映了其重要性

**相关系数排序：**

特征被选择的顺序大致按照 $|\mathbf{x}_j'\mathbf{y}|$ 的大小排序。

### 5.3 特征选择的稳定性

**问题：**

当特征高度相关时：
- Lasso 可能随机选择其中一个
- 结果不稳定
- 可能丢失重要信息

**解决方案：**

1. **Elastic Net**：结合 L1 和 L2 正则化
2. **Group Lasso**：将相关特征分组
3. **稳定性选择**：使用 Bootstrap 提高稳定性

---

## 六、正则化路径

### 6.1 什么是正则化路径

**定义：** 正则化路径是指随着 $\lambda$ 的变化，参数估计值 $\hat{\boldsymbol{\beta}}(\lambda)$ 的变化轨迹。

### 6.2 Lasso 路径的性质

**分段线性性：**

Lasso 路径是分段线性的：
- 在相邻的"事件点"之间，路径是线性的
- "事件点"是特征被选择或移除的时刻

**事件点：**

1. **加入事件**：某个特征的相关系数达到 $\lambda$
2. **移除事件**：某个特征的系数变为 0

### 6.3 路径的计算

**LARS 算法：**

LARS 算法可以高效地计算整个正则化路径。

**算法输出：**
- 所有事件点的 $\lambda$ 值
- 每个区间的参数值
- 完整的路径

### 6.4 路径的可视化

**典型路径图：**

- 横轴：$\lambda$（或 $\log(\lambda)$）
- 纵轴：参数值 $\beta_j$
- 每条线代表一个参数

**观察：**
- 参数从 0 开始（$\lambda$ 很大时）
- 随着 $\lambda$ 减小，参数逐渐非零
- 某些参数可能再次变为 0

---

## 七、Lasso 的变体

### 7.1 Adaptive Lasso

**Adaptive Lasso** 使用自适应权重：

$$
\min_{\boldsymbol{\beta}} \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda\sum_{j=1}^{p}w_j|\beta_j|
$$

其中 $w_j$ 是自适应权重，通常：

$$
w_j = |\hat{\beta}_j^{\text{OLS}}|^{-\gamma}
$$

**优势：**
- 对重要特征惩罚更小
- 提高特征选择的一致性

### 7.2 Group Lasso

**Group Lasso** 将特征分组：

$$
\min_{\boldsymbol{\beta}} \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda\sum_{g=1}^{G}\sqrt{p_g}\|\boldsymbol{\beta}_g\|_2
$$

其中 $G$ 是组数，$p_g$ 是第 $g$ 组的特征数。

**特点：**
- 以组为单位选择特征
- 适用于特征有分组结构的情况

### 7.3 Fused Lasso

**Fused Lasso** 惩罚相邻参数的差异：

$$
\min_{\boldsymbol{\beta}} \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda_1\sum_{j=1}^{p}|\beta_j| + \lambda_2\sum_{j=2}^{p}|\beta_j - \beta_{j-1}|
$$

**应用：**
- 时间序列数据
- 需要平滑性的场景

### 7.4 Elastic Net

**Elastic Net** 结合 L1 和 L2 正则化：

$$
\min_{\boldsymbol{\beta}} \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda\left[\alpha\|\boldsymbol{\beta}\|_1 + (1-\alpha)\frac{1}{2}\|\boldsymbol{\beta}\|_2^2\right]
$$

**优势：**
- 结合 Lasso 和 Ridge 的优点
- 对相关特征更稳定

---

## 八、超参数选择

### 8.1 交叉验证

**K 折交叉验证：**

1. 将数据分为 $K$ 折
2. 对每个 $\lambda$ 值：
   - 用 $K-1$ 折训练模型
   - 用剩余 1 折验证
   - 计算平均验证误差
3. 选择验证误差最小的 $\lambda$

**留一交叉验证（LOOCV）：**
- 特殊情况：$K = n$
- 计算量大，但无偏

### 8.2 信息准则

**AIC（Akaike Information Criterion）：**

$$
\text{AIC} = 2k - 2\ln(L)
$$

其中 $k$ 是选择的特征数。

**BIC（Bayesian Information Criterion）：**

$$
\text{BIC} = k\ln(n) - 2\ln(L)
$$

BIC 对模型复杂度惩罚更重。

### 8.3 正则化参数的选择范围

**常用范围：**
- $\lambda \in [10^{-4}, 10^2]$（对数尺度）
- 使用网格搜索
- 通常在对数尺度上均匀采样

**选择策略：**
- 从大到小搜索
- 使用 LARS 计算路径
- 选择验证误差最小的点

---

## 九、在量化交易中的应用

### 9.1 因子选择

**应用场景：**

从大量候选因子中选择有效因子：

**步骤：**
1. 收集大量技术指标和基本面指标
2. 使用 Lasso 回归选择重要因子
3. 构建基于选择因子的预测模型

**优势：**
- 自动选择重要因子
- 减少过拟合风险
- 提高模型可解释性

### 9.2 特征工程

**应用场景：**

在特征工程阶段使用 Lasso 进行特征选择：

**方法：**
1. 创建大量特征（技术指标、交互项等）
2. 使用 Lasso 选择重要特征
3. 用选择的特征训练最终模型

**优势：**
- 自动特征选择
- 减少特征维度
- 提高模型性能

### 9.3 高维因子模型

**应用场景：**

当因子数量远大于样本数量时（$p >> n$）：

**方法：**
1. 使用 Lasso 选择重要因子
2. 构建稀疏的因子模型
3. 估计因子暴露

**优势：**
- 处理高维数据
- 选择重要因子
- 提高模型稳定性

### 9.4 风险模型

**应用场景：**

构建风险因子模型：

**方法：**
1. 使用 Lasso 选择风险因子
2. 估计资产的风险暴露
3. 进行风险分解

**优势：**
- 识别重要风险因子
- 构建稀疏风险模型
- 提高风险估计精度

### 9.5 实际应用注意事项

**1. 数据预处理**
- 必须进行特征标准化
- 处理缺失值
- 处理异常值

**2. 时间序列特性**
- 使用时间序列交叉验证
- 避免数据泄露
- 注意时间依赖性

**3. 特征稳定性**
- 检查特征选择的稳定性
- 使用 Bootstrap 或稳定性选择
- 定期重新评估特征

**4. 过拟合控制**
- 使用交叉验证选择 $\lambda$
- 检查样本外性能
- 监控模型性能衰减

---

## 十、Python 实现示例

### 10.1 基础 Lasso 回归

```python
from sklearn.linear_model import Lasso, LassoCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 数据标准化（必须！）
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Lasso 回归
lasso = Lasso(alpha=0.1, max_iter=1000)
lasso.fit(X_train_scaled, y_train)

# 预测
y_pred = lasso.predict(X_test_scaled)

# 评估
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"MSE: {mse:.4f}")
print(f"R²: {r2:.4f}")
print(f"选择的特征数: {np.sum(lasso.coef_ != 0)}")
```

### 10.2 交叉验证选择参数

```python
# 使用交叉验证选择最优 alpha
alphas = np.logspace(-4, 1, 100)  # 10^-4 到 10^1

lasso_cv = LassoCV(
    alphas=alphas,
    cv=5,  # 5 折交叉验证
    random_state=42,
    max_iter=1000,
    n_jobs=-1
)

lasso_cv.fit(X_train_scaled, y_train)

print(f"最优 alpha: {lasso_cv.alpha_:.6f}")
print(f"交叉验证 MSE: {np.min(lasso_cv.mse_path_.mean(axis=1)):.4f}")

# 使用最优参数预测
y_pred = lasso_cv.predict(X_test_scaled)
```

### 10.3 正则化路径可视化

```python
from sklearn.linear_model import lasso_path

# 计算正则化路径
alphas_path, coefs_path, _ = lasso_path(
    X_train_scaled, y_train,
    alphas=alphas,
    max_iter=1000
)

# 绘制路径
plt.figure(figsize=(12, 8))
plt.plot(np.log10(alphas_path), coefs_path.T)
plt.xlabel('log10(alpha)', fontsize=12)
plt.ylabel('系数', fontsize=12)
plt.title('Lasso 正则化路径', fontsize=14)
plt.axvline(x=np.log10(lasso_cv.alpha_), color='r', 
           linestyle='--', label=f'最优 alpha={lasso_cv.alpha_:.4f}')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

### 10.4 特征重要性分析

```python
# 选择的特征和系数
selected_features = pd.DataFrame({
    'feature': feature_names,
    'coefficient': lasso_cv.coef_,
    'abs_coefficient': np.abs(lasso_cv.coef_)
})

# 只保留非零系数
selected_features = selected_features[selected_features['coefficient'] != 0]
selected_features = selected_features.sort_values('abs_coefficient', ascending=False)

print("选择的特征:")
print(selected_features)

# 可视化
plt.figure(figsize=(10, 6))
plt.barh(range(len(selected_features)), selected_features['coefficient'])
plt.yticks(range(len(selected_features)), selected_features['feature'])
plt.xlabel('系数', fontsize=12)
plt.title('Lasso 选择的特征及其系数', fontsize=14)
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()
```

### 10.5 量化交易应用示例

```python
import yfinance as yf
import pandas as pd
import numpy as np
from sklearn.linear_model import LassoCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score

def get_stock_data(symbol, period='3y'):
    """获取股票数据"""
    stock = yf.Ticker(symbol)
    data = stock.history(period=period)
    return data

def calculate_features(data):
    """计算大量技术指标特征"""
    df = data.copy()
    
    # 收益率
    df['Return'] = df['Close'].pct_change()
    
    # RSI（多个周期）
    for period in [7, 14, 21]:
        delta = df['Close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        df[f'RSI_{period}'] = 100 - (100 / (1 + rs))
    
    # MACD（多个组合）
    for fast, slow in [(12, 26), (5, 13), (19, 39)]:
        exp1 = df['Close'].ewm(span=fast, adjust=False).mean()
        exp2 = df['Close'].ewm(span=slow, adjust=False).mean()
        df[f'MACD_{fast}_{slow}'] = exp1 - exp2
    
    # 均线（多个周期）
    for window in [5, 10, 20, 50, 100, 200]:
        df[f'MA_{window}'] = df['Close'].rolling(window=window).mean()
        df[f'MA_{window}_ratio'] = df['Close'] / df[f'MA_{window}']
        df[f'MA_{window}_diff'] = df['Close'] - df[f'MA_{window}']
    
    # 波动率（多个周期）
    for window in [5, 10, 20, 30, 60]:
        df[f'Volatility_{window}'] = df['Return'].rolling(window=window).std()
    
    # 成交量指标
    for window in [5, 10, 20, 50]:
        df[f'Volume_MA_{window}'] = df['Volume'].rolling(window=window).mean()
        df[f'Volume_Ratio_{window}'] = df['Volume'] / df[f'Volume_MA_{window}']
    
    # 动量（多个周期）
    for period in [1, 3, 5, 10, 20, 30, 60]:
        df[f'Momentum_{period}'] = df['Close'].pct_change(period)
    
    # 价格位置
    df['High_Low_Ratio'] = (df['High'] - df['Low']) / df['Close']
    df['Open_Close_Ratio'] = df['Open'] / df['Close']
    
    return df

def main():
    # 获取数据
    data = get_stock_data('AAPL', period='3y')
    
    # 计算特征
    data = calculate_features(data)
    
    # 创建目标变量（未来收益率）
    data['Future_Return'] = data['Return'].shift(-1)
    
    # 选择特征
    feature_cols = [col for col in data.columns 
                   if col not in ['Future_Return', 'Return', 'Close', 'High', 
                                 'Low', 'Open', 'Volume', 'Dividends', 'Stock Splits']]
    
    # 删除缺失值
    data = data[feature_cols + ['Future_Return']].dropna()
    
    X = data[feature_cols].values
    y = data['Future_Return'].values
    
    print(f"特征数量: {X.shape[1]}")
    print(f"样本数量: {X.shape[0]}")
    
    # 时间序列划分
    split_idx = int(len(X) * 0.8)
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    # 标准化
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Lasso 回归（特征选择）
    print("\n" + "=" * 60)
    print("Lasso 回归 - 特征选择")
    print("=" * 60)
    
    alphas = np.logspace(-4, 1, 100)
    lasso_cv = LassoCV(
        alphas=alphas,
        cv=5,
        random_state=42,
        max_iter=2000,
        n_jobs=-1
    )
    
    lasso_cv.fit(X_train_scaled, y_train)
    
    # 预测
    y_pred = lasso_cv.predict(X_test_scaled)
    
    # 评估
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    
    print(f"最优 alpha: {lasso_cv.alpha_:.6f}")
    print(f"选择的特征数: {np.sum(lasso_cv.coef_ != 0)} / {len(feature_cols)}")
    print(f"测试集 MSE: {mse:.6f}")
    print(f"测试集 R²: {r2:.4f}")
    
    # 选择的特征
    selected_features = pd.DataFrame({
        'feature': feature_cols,
        'coefficient': lasso_cv.coef_,
        'abs_coefficient': np.abs(lasso_cv.coef_)
    })
    
    selected_features = selected_features[selected_features['coefficient'] != 0]
    selected_features = selected_features.sort_values('abs_coefficient', ascending=False)
    
    print(f"\n选择的 {len(selected_features)} 个特征:")
    print(selected_features[['feature', 'coefficient']].to_string(index=False))
    
    return lasso_cv, selected_features, scaler

if __name__ == '__main__':
    model, features, scaler = main()
```

### 10.6 稳定性选择

```python
from sklearn.utils import resample

def stability_selection(X, y, n_bootstrap=100, alpha=0.1):
    """稳定性选择：使用 Bootstrap 提高特征选择的稳定性"""
    n_samples, n_features = X.shape
    feature_frequencies = np.zeros(n_features)
    
    for _ in range(n_bootstrap):
        # Bootstrap 采样
        indices = resample(range(n_samples), n_samples=n_samples, random_state=None)
        X_boot = X[indices]
        y_boot = y[indices]
        
        # Lasso 回归
        lasso = Lasso(alpha=alpha, max_iter=1000)
        lasso.fit(X_boot, y_boot)
        
        # 记录选择的特征
        feature_frequencies += (lasso.coef_ != 0).astype(int)
    
    # 计算选择频率
    feature_frequencies = feature_frequencies / n_bootstrap
    
    return feature_frequencies

# 使用稳定性选择
frequencies = stability_selection(X_train_scaled, y_train, n_bootstrap=100, alpha=0.1)

# 选择频率高的特征
threshold = 0.5  # 选择频率 > 50% 的特征
stable_features = np.where(frequencies > threshold)[0]

print(f"稳定性选择: {len(stable_features)} 个特征")
print(f"特征索引: {stable_features}")
```

---

## 十一、总结

### 11.1 核心要点

**1. Lasso 回归的本质**
- 使用 L1 正则化约束模型参数
- 可以将参数压缩到 0，实现特征选择
- 产生稀疏模型

**2. 核心优势**
- 自动特征选择
- 处理高维数据
- 提高模型可解释性
- 减少过拟合

**3. 关键性质**
- 稀疏性：参数中很多为 0
- 最多选择 $n$ 个特征（当 $p > n$ 时）
- 对相关特征不稳定

### 11.2 优势与局限

**优势：**
- ✅ 自动特征选择
- ✅ 稀疏模型，易于解释
- ✅ 处理高维数据
- ✅ 计算效率高

**局限：**
- ❌ 对相关特征不稳定
- ❌ 最多选择 $n$ 个特征
- ❌ 需要选择正则化参数
- ❌ 需要数据标准化

### 11.3 在量化交易中的应用

**主要应用：**
1. **因子选择**：从大量因子中选择有效因子
2. **特征工程**：自动选择重要特征
3. **高维建模**：处理 $p >> n$ 的情况
4. **风险模型**：构建稀疏风险因子模型

**注意事项：**
- 必须进行特征标准化
- 使用交叉验证选择参数
- 注意特征选择的稳定性
- 使用时间序列交叉验证

### 11.4 学习建议

**1. 理论基础**
- 理解 L1 正则化的几何意义
- 理解软阈值算子
- 理解特征选择的机制

**2. 实践应用**
- 使用 scikit-learn 实现 Lasso
- 进行特征选择实验
- 在量化交易中应用 Lasso

**3. 进阶学习**
- 学习 Lasso 的变体（Adaptive Lasso、Group Lasso）
- 学习稳定性选择
- 学习其他特征选择方法

---

**总结：Lasso 回归是一种强大的正则化方法，通过 L1 正则化实现自动特征选择，产生稀疏模型。在量化交易中，Lasso 被广泛应用于因子选择、特征工程和高维建模。理解 Lasso 的原理、算法和应用，对于构建有效的量化模型至关重要。通过合理使用交叉验证、稳定性选择等方法，可以提高特征选择的稳定性和模型的泛化能力。**

